{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sequence Tagging\n",
    "\n",
    "In this lab we will train a part-of-speech (POS) tagger using an HMM and then an RNN.\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "- Be able to train and apply an HMM.\n",
    "- Understand what the steps of Viterbi are doing.\n",
    "- Recognise how to adapt Pytorch models to use RNN layers and perform sequence tagging with neural networks.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads a POS dataset from the NLTK library.\n",
    "The second part implements and tests an HMM POS tagger.\n",
    "The third part adapts the neural network code from last week to train the RNN as a POS tagger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the PoS Tagging Data\n",
    "\n",
    "To train our POS tagger, we will use the Brown corpus, which contains many different sources of English text (books, essays, newspaper articles, government documents...) collected and hand-labelled by linguists in 1967.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the dataset into train and test, then re-format it so that each split is represented by a list of sentences and a list of tag sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use LabelEncoder to map the tokens to IDs and convert the sentences to sequences of token IDs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final preprocessing step is to map the tags (class labels) to numerical IDs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 10:30:42,804 brown downloading brown\n",
      "2023-11-15 10:30:42,888 brown downloading universal_tagset\n",
      "2023-11-15 10:30:46,047 brown 57340 sentences\n",
      "2023-11-15 10:30:54,795 brown 56057 words\n",
      "2023-11-15 10:30:54,795 brown 12 tags\n",
      "2023-11-15 10:30:54,826 pickle dumping brown_tagged_sentences.pickle\n",
      "2023-11-15 10:30:57,081 pickle dumped brown_tagged_sentences.pickle\n"
     ]
    }
   ],
   "source": [
    "from dn.sequence_tagging.brown import get_brown_tagged_sentences\n",
    "\n",
    "brown_tagged_sentences = get_brown_tagged_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Implementing the HMM\n",
    "\n",
    "Now, we are going to put together an HMM by estimating the different variables in the model from the training set.\n",
    "\n",
    "**TO-DO 2.1:** Count the state transitions and starting state occurrences in the training set and store the counts in the `transitions` and `start_states` matrices below. In `transitions`, rows correspond to states at time t-1, the columns to the following state at time t.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.2:** Normalise the transition and start state counts to estimate the conditional probabilities in the transition matrix and \\pi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.3:** Count the number of occurrences of each word type given each tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 10:30:57,096 hmm __initial_matrix (12,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 10:30:57,251 hmm __transition_matrix (12, 12)\n",
      "2023-11-15 10:30:57,424 hmm __emission_matrix (12, 56057)\n"
     ]
    }
   ],
   "source": [
    "from numpy import int64\n",
    "from dn.sequence_tagging.hmm import HMMTagger\n",
    "\n",
    "hmm_tagger = HMMTagger[int64](\n",
    "    brown_tagged_sentences.n_words, brown_tagged_sentences.n_tags\n",
    ")\n",
    "\n",
    "hmm_tagger.fit(\n",
    "    brown_tagged_sentences.words_train_encoded,\n",
    "    brown_tagged_sentences.tags_train_encoded,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.4:** Normalise the observation counts to obtain the observation probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.5:** Check the implementation of viterbi below for errors!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.6:** Use the viterbi function to estimate the most likely sequence of states on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will convert the predicted tag IDs to names and print the predictions along with ground truth for selected examples so we can see where it made errors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open market policy\n",
      "ADJ NOUN NOUN\n",
      "NOUN NOUN NOUN\n",
      "And you think you have language problems .\n",
      "CONJ PRON VERB PRON VERB NOUN NOUN .\n",
      "CONJ PRON VERB PRON VERB NOUN NOUN .\n",
      "Mae entered the room from the hallway to the kitchen .\n",
      "NOUN VERB DET NOUN ADP DET NOUN ADP DET NOUN .\n",
      "NOUN VERB DET NOUN ADP DET NOUN PRT DET NOUN .\n",
      "This will permit you to get a rough estimate of how much the materials for the shell will cost .\n",
      "DET VERB VERB PRON PRT VERB DET ADJ NOUN ADP ADV ADJ DET NOUN ADP DET NOUN VERB VERB .\n",
      "DET VERB VERB PRON PRT VERB DET ADJ NOUN ADP ADV ADV DET NOUN ADP DET NOUN VERB NOUN .\n",
      "the multifigure `` Traveling Carnival '' , in which action is vivified by lighting ; ;\n",
      "DET NOUN . VERB NOUN . . ADP DET NOUN VERB VERB ADP VERB . .\n",
      "DET NOUN . VERB NOUN . . ADP DET NOUN VERB NOUN ADP NOUN . .\n"
     ]
    }
   ],
   "source": [
    "# Print the predicted tags for the first N sentences.\n",
    "N_SENTENCES = 5\n",
    "for test_words, test_tags in zip(\n",
    "    brown_tagged_sentences.words_test_encoded[:N_SENTENCES],\n",
    "    brown_tagged_sentences.tags_test_encoded[:N_SENTENCES],\n",
    "):\n",
    "    pred_tags = list(\n",
    "        hmm_tagger.predict(\n",
    "            test_words,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    test_words_decoded = brown_tagged_sentences.decode_words(test_words)\n",
    "    test_tags_decoded = brown_tagged_sentences.decode_tags(test_tags)\n",
    "    pred_tags_decoded = brown_tagged_sentences.decode_tags(pred_tags)\n",
    "\n",
    "    print(\" \".join(test_words_decoded))\n",
    "    print(\" \".join(test_tags_decoded))\n",
    "    print(\" \".join(pred_tags_decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well it did overall by computing performance metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.417\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "# Compute the accuracy for the test set.\n",
    "correct: int = 0\n",
    "for test_words, test_tags in zip(\n",
    "    brown_tagged_sentences.words_test_encoded,\n",
    "    brown_tagged_sentences.tags_test_encoded,\n",
    "):\n",
    "    pred_tags = list(\n",
    "        hmm_tagger.predict(\n",
    "            test_words,\n",
    "        )\n",
    "    )\n",
    "    correct += numpy.sum(pred_tags == test_tags)\n",
    "\n",
    "accuracy = correct / len(brown_tagged_sentences.tags_test_encoded)\n",
    "print(f\"accuracy {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. POS Tagging with an RNN\n",
    "\n",
    "The code below is adapted from last week's text classifier code to first pad the sequences, then format them into DataLoader objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 10:31:04,042 pickle loading brown_tagged_sentences.pickle\n",
      "2023-11-15 10:31:04,472 pickle loaded brown_tagged_sentences.pickle\n",
      "2023-11-15 10:31:04,778 rnn words_train_padded (45872, 40)\n",
      "2023-11-15 10:31:04,873 rnn words_test_padded (11468, 40)\n",
      "2023-11-15 10:31:05,151 rnn tags_train_padded (45872, 40)\n",
      "2023-11-15 10:31:05,232 rnn tags_test_padded (11468, 40)\n",
      "2023-11-15 10:31:05,254 pickle dumping brown_tagged_sentences_padded.pickle\n",
      "2023-11-15 10:31:05,265 pickle dumped brown_tagged_sentences_padded.pickle\n"
     ]
    }
   ],
   "source": [
    "from dn.sequence_tagging.rnn import get_brown_tagged_sentences_padded\n",
    "\n",
    "(\n",
    "    words_train_padded,\n",
    "    words_test_padded,\n",
    "    tags_train_padded,\n",
    "    tags_test_padded,\n",
    "    n_words,\n",
    "    n_tags,\n",
    ") = get_brown_tagged_sentences_padded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dn.sequence_tagging.util import to_data_loader\n",
    "\n",
    "data_loader_train = to_data_loader(words_train_padded, tags_train_padded)\n",
    "data_loader_test = to_data_loader(words_test_padded, tags_test_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we're going to create a neural network sequence tagger using an RNN layer. This will be based on the code we used last time, with two key differences:\n",
    "\n",
    "- Including an RNN hidden layer\n",
    "- The output will have an additional dimension of size sequence_length, so that it can provide predictions for every token in the sequence.\n",
    "\n",
    "**TODO 3.1:** Complete the code below to change the hidden layer to a single RNN layer. See [the documentation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we can run the code below to train and test the RNN model. This uses basically the same code as last week.\n",
    "\n",
    "**TO-DO 3.2:** What is wrong with comparing the RNN tagger's performance computed here with that of the HMM? Hint: all the sequences are length 40.\n",
    "\n",
    "**TO-DO 3.3:** Can you fix the accuracy computations to make them comparable with the accuracy for the HMM?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs the trainin process by calling train_nn():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements a testing or prediction function and computes accuracy.\n",
    "\n",
    "**TO-DO 3.4:** Adjust the code below to correctly compute the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 717/717 [00:03<00:00, 187.61it/s]\n",
      "2023-11-15 10:31:09,385 rnn epoch 1\n",
      "2023-11-15 10:31:09,385 rnn training loss 1.412\n",
      "2023-11-15 10:31:09,386 rnn training accuracy 27.1%\n",
      "2023-11-15 10:31:09,650 rnn validation loss 0.893\n",
      "2023-11-15 10:31:09,650 rnn validation accuracy 34.5%\n",
      "100%|██████████| 717/717 [00:03<00:00, 187.13it/s]\n",
      "2023-11-15 10:31:13,483 rnn epoch 2\n",
      "2023-11-15 10:31:13,483 rnn training loss 0.716\n",
      "2023-11-15 10:31:13,484 rnn training accuracy 37.2%\n",
      "2023-11-15 10:31:13,727 rnn validation loss 0.592\n",
      "2023-11-15 10:31:13,728 rnn validation accuracy 39.1%\n",
      "100%|██████████| 717/717 [00:03<00:00, 190.29it/s]\n",
      "2023-11-15 10:31:17,497 rnn epoch 3\n",
      "2023-11-15 10:31:17,497 rnn training loss 0.511\n",
      "2023-11-15 10:31:17,497 rnn training accuracy 40.4%\n",
      "2023-11-15 10:31:17,751 rnn validation loss 0.457\n",
      "2023-11-15 10:31:17,751 rnn validation accuracy 41.2%\n",
      "100%|██████████| 717/717 [00:03<00:00, 188.85it/s]\n",
      "2023-11-15 10:31:21,548 rnn epoch 4\n",
      "2023-11-15 10:31:21,549 rnn training loss 0.405\n",
      "2023-11-15 10:31:21,549 rnn training accuracy 42.0%\n",
      "2023-11-15 10:31:21,787 rnn validation loss 0.380\n",
      "2023-11-15 10:31:21,788 rnn validation accuracy 42.4%\n",
      "100%|██████████| 717/717 [00:03<00:00, 190.50it/s]\n",
      "2023-11-15 10:31:25,552 rnn epoch 5\n",
      "2023-11-15 10:31:25,553 rnn training loss 0.338\n",
      "2023-11-15 10:31:25,553 rnn training accuracy 43.1%\n",
      "2023-11-15 10:31:25,805 rnn validation loss 0.329\n",
      "2023-11-15 10:31:25,805 rnn validation accuracy 43.2%\n",
      "100%|██████████| 717/717 [00:03<00:00, 187.80it/s]\n",
      "2023-11-15 10:31:29,624 rnn epoch 6\n",
      "2023-11-15 10:31:29,624 rnn training loss 0.291\n",
      "2023-11-15 10:31:29,625 rnn training accuracy 43.9%\n",
      "2023-11-15 10:31:29,858 rnn validation loss 0.293\n",
      "2023-11-15 10:31:29,859 rnn validation accuracy 43.8%\n",
      "100%|██████████| 717/717 [00:03<00:00, 189.40it/s]\n",
      "2023-11-15 10:31:33,645 rnn epoch 7\n",
      "2023-11-15 10:31:33,646 rnn training loss 0.255\n",
      "2023-11-15 10:31:33,646 rnn training accuracy 44.5%\n",
      "2023-11-15 10:31:33,896 rnn validation loss 0.265\n",
      "2023-11-15 10:31:33,897 rnn validation accuracy 44.2%\n",
      "100%|██████████| 717/717 [00:03<00:00, 189.12it/s]\n",
      "2023-11-15 10:31:37,689 rnn epoch 8\n",
      "2023-11-15 10:31:37,689 rnn training loss 0.228\n",
      "2023-11-15 10:31:37,689 rnn training accuracy 44.9%\n",
      "2023-11-15 10:31:37,924 rnn validation loss 0.245\n",
      "2023-11-15 10:31:37,925 rnn validation accuracy 44.6%\n",
      "100%|██████████| 717/717 [00:03<00:00, 189.65it/s]\n",
      "2023-11-15 10:31:41,706 rnn epoch 9\n",
      "2023-11-15 10:31:41,707 rnn training loss 0.206\n",
      "2023-11-15 10:31:41,707 rnn training accuracy 45.3%\n",
      "2023-11-15 10:31:41,957 rnn validation loss 0.229\n",
      "2023-11-15 10:31:41,957 rnn validation accuracy 44.9%\n",
      "100%|██████████| 717/717 [00:03<00:00, 190.97it/s]\n",
      "2023-11-15 10:31:45,713 rnn epoch 10\n",
      "2023-11-15 10:31:45,713 rnn training loss 0.188\n",
      "2023-11-15 10:31:45,714 rnn training accuracy 45.6%\n",
      "2023-11-15 10:31:45,948 rnn validation loss 0.215\n",
      "2023-11-15 10:31:45,948 rnn validation accuracy 45.1%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dn.sequence_tagging.rnn import RNNTagger\n",
    "\n",
    "EMBEDDING_DIM = 25\n",
    "HIDDEN_DIM = 32\n",
    "HIDDEN_LAYERS = 1\n",
    "LEARNING_RATE = 0.0005\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Ignore the padding index when computing the loss.\n",
    "cross_entropy_loss = torch.nn.CrossEntropyLoss(ignore_index=n_words)\n",
    "\n",
    "rnn_tagger = RNNTagger(\n",
    "    loss_fn=cross_entropy_loss,\n",
    "    # Include the padding index in the input.\n",
    "    n_words=n_words + 1,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    hidden_layers=HIDDEN_LAYERS,\n",
    "    # Include the padding index in the output.\n",
    "    output_dim=n_tags + 1,\n",
    ")\n",
    "\n",
    "adam_optimizer = torch.optim.Adam(rnn_tagger.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "rnn_tagger.train_(\n",
    "    n_epochs=N_EPOCHS,\n",
    "    train_loader=data_loader_train,\n",
    "    val_loader=data_loader_test,\n",
    "    optimizer=adam_optimizer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
