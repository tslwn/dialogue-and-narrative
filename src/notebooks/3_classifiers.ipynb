{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "This lab explores a new dataset for text classification tasks using naïve Bayes and logistic regression.\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "- Train and test naive_bayes and LR classifiers using an established library.\n",
    "- Apply evaluation metrics to the classifiers and display examples of misclassifications.\n",
    "- Examine learned model parameters to explain how each classifier makes a decision.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads a new Twitter dataset, which is described in [this paper](https://arxiv.org/pdf/2010.12421.pdf), then extracts feature vectors from each sample.\n",
    "The next part involves implementing and evaluating the classifiers using Scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tweet_eval (/Users/qr23940/git/dialogue_and_narrative/src/notebooks/data_cache/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "Found cached dataset tweet_eval (/Users/qr23940/git/dialogue_and_narrative/src/notebooks/data_cache/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    }
   ],
   "source": [
    "from dn.datasets import TweetEvalDataset\n",
    "\n",
    "train = TweetEvalDataset(\"sentiment\", \"train\")\n",
    "test = TweetEvalDataset(\"sentiment\", \"test\")\n",
    "\n",
    "train_texts: list[str] = []\n",
    "train_labels: list[int] = []\n",
    "\n",
    "for item in train.iter():\n",
    "    train_texts.append(item[\"text\"])\n",
    "    train_labels.append(item[\"label\"])\n",
    "\n",
    "test_texts: list[str] = []\n",
    "test_labels: list[int] = []\n",
    "\n",
    "for item in test.iter():\n",
    "    test_texts.append(item[\"text\"])\n",
    "    test_labels.append(item[\"label\"])\n",
    "\n",
    "train_texts = train_texts[:1000]\n",
    "train_labels = train_labels[:1000]\n",
    "test_texts = test_texts[:1000]\n",
    "test_labels = test_labels[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert the tokenised text of each tweet to a feature vectors that we can use as input to a classifier. The feature vector needs to be a numerical vector of a fixed size. For the bag-of-words representation, the feature vector for a tweet will represent the number of occurrences of each word in the vocabulary in that tweet.\n",
    "\n",
    "For this, we can use the CountVectorizer class: [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "**TO DO 1.1:** Why do we need to fit the CountVectorizer on the train set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dn.embeddings.document_term_matrix import DocumentTermMatrix\n",
    "\n",
    "document_term_matrix = DocumentTermMatrix(train_texts)\n",
    "document_term_matrix_test = document_term_matrix.transform(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Naive Bayes Classifier\n",
    "\n",
    "The code above has obtained the feature vectors and lists of labels. The data is now ready for use\n",
    "with scikit-learn's classifiers.\n",
    "\n",
    "**TODO 2.1:** Train a classifier using the [MultinomialNB class.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) You will need to look at the linked documentation to see how to construct and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyright: reportUnknownMemberType=false\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(document_term_matrix.array, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO 2.2:** Again use the documentation to write code to obtain predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "\n",
    "pred_labels_naive_bayes = naive_bayes.predict(document_term_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO 2.3:** Compute accuracy, precision, recall and F1 scores on the test set using [scikit-learn's metrics libary.](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules) Review the documentation to see the different options for evaluating classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.443\n",
      "precision = 0.425\n",
      "recall = 0.422\n",
      "f1 = 0.354\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "\n",
    "from typing import Any, Literal\n",
    "import numpy.typing\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "Average = Literal[\"micro\", \"macro\", \"samples\", \"weighted\", \"binary\"] | None\n",
    "\n",
    "\n",
    "def print_metrics(\n",
    "    test_labels: list[int],\n",
    "    pred_labels: numpy.typing.NDArray[Any],\n",
    "    average: Average = \"macro\",\n",
    "):\n",
    "    accuracy = accuracy_score(test_labels, pred_labels)\n",
    "    print(f\"accuracy = {accuracy:.3f}\")\n",
    "    precision = precision_score(test_labels, pred_labels, average=average)\n",
    "    print(f\"precision = {precision:.3f}\")\n",
    "    recall = recall_score(test_labels, pred_labels, average=average)\n",
    "    print(f\"recall = {recall:.3f}\")\n",
    "    f1 = f1_score(test_labels, pred_labels, average=average)\n",
    "    print(f\"f1 = {f1:.3f}\")\n",
    "\n",
    "\n",
    "print_metrics(test_labels, pred_labels_naive_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO 2.4:** Print out the ten features with the strongest association with each class. Hint: use the `feature_log_prob_` attribute of the MultinomialNB object. You may also need Numpy's argsort() function.\n",
    "\n",
    "Beware offensive words below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class = 0\n",
      "  ira = 0.770\n",
      "  french = 0.767\n",
      "  wrong = 0.767\n",
      "  name = 0.737\n",
      "  marriage = 0.718\n",
      "  300 = 0.712\n",
      "  airasia = 0.712\n",
      "  arabia = 0.712\n",
      "  bnp = 0.712\n",
      "  britain = 0.712\n",
      "class = 1\n",
      "  rollins = 0.758\n",
      "  delhi = 0.736\n",
      "  plan = 0.736\n",
      "  dec = 0.677\n",
      "  gandhi = 0.677\n",
      "  sox = 0.676\n",
      "  january = 0.667\n",
      "  half = 0.662\n",
      "  seth = 0.661\n",
      "  10 = 0.639\n",
      "class = 2\n",
      "  happy = 0.907\n",
      "  birthday = 0.891\n",
      "  amazing = 0.786\n",
      "  love = 0.767\n",
      "  thank = 0.767\n",
      "  excited = 0.766\n",
      "  fun = 0.766\n",
      "  great = 0.750\n",
      "  foo = 0.741\n",
      "  mcgregor = 0.741\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "\n",
    "import numpy\n",
    "import numpy.typing\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def print_top_features_log_prob(\n",
    "    feature_names: numpy.typing.NDArray[Any],\n",
    "    classes: numpy.typing.NDArray[Any],\n",
    "    classes_feature_log_probs: numpy.typing.NDArray[Any],\n",
    "    n: int = 10,\n",
    "):\n",
    "    # Find the probability of the features over all classes.\n",
    "    feature_probs = numpy.sum(numpy.exp(classes_feature_log_probs), axis=0)\n",
    "\n",
    "    # For each class...\n",
    "    for class_name, class_feature_log_probs in zip(\n",
    "        classes, classes_feature_log_probs\n",
    "    ):\n",
    "        print(f\"class = {class_name}\")\n",
    "\n",
    "        # Find the probability of the features for the class.\n",
    "        class_feature_probs = numpy.vectorize(numpy.exp)(\n",
    "            class_feature_log_probs\n",
    "        )\n",
    "\n",
    "        features = sorted(\n",
    "            (\n",
    "                # Find the ratio of the probabilities of the feature for the\n",
    "                # class and over all classes.\n",
    "                (feature_name, class_feature_prob / feature_prob)\n",
    "                for feature_name, class_feature_prob, feature_prob in zip(\n",
    "                    feature_names,\n",
    "                    class_feature_probs,\n",
    "                    feature_probs,\n",
    "                )\n",
    "            ),\n",
    "            key=lambda feature: feature[1],\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        # Print the N features with the highest ratios.\n",
    "        for feature, prob_ratio in features[:n]:\n",
    "            print(f\"  {feature} = {prob_ratio:.3f}\")\n",
    "\n",
    "\n",
    "print_top_features_log_prob(\n",
    "    document_term_matrix.get_feature_names(),\n",
    "    naive_bayes.classes_,\n",
    "    naive_bayes.feature_log_prob_,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Performance metrics are just one of the ways that we need to evaluate classifiers. Metrics summarise the performance of a classifier across many different examples in the test set, but they don't tell us what the model is good at, or what kind of mistakes it makes. For this, we need to examine the errors it makes, and try to identify patterns -- this helps us to come up with improvements to the model.\n",
    "\n",
    "**TODO 2.5:** As a first error analysis step, print out some examples of misclassified tweets, along with their predicted and true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label = 1, predicted label = 2\n",
      "OH: “I had a blue penis while I was this” [playing with Google Earth VR]\n",
      "true label = 2, predicted label = 1\n",
      "I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
      "true label = 0, predicted label = 1\n",
      "@user Wow,first Hugo Chavez and now Fidel Castro. Danny Glover, Michael Moore, Oliver Stone, and Sean Penn are running out of heroes.\n",
      "true label = 1, predicted label = 2\n",
      "Savchenko now Saakashvili took drug test live on Ukraine TV. To prove they are not drug-fueled loonies?\n",
      "true label = 1, predicted label = 2\n",
      "How many more days until opening day? 😩\n",
      "true label = 2, predicted label = 1\n",
      "Twitter's #ThankYouObama Shows Heartfelt Gratitude To POTUS\n",
      "true label = 0, predicted label = 1\n",
      "@user @user @user @user @user @user take away illegals and dead people and Trump wins popular vote too.\n",
      "true label = 1, predicted label = 2\n",
      "An interesting security vulnerability - albeit not for the everyday car thief\n",
      "true label = 0, predicted label = 1\n",
      "When Ryan privatizes SS, Medicare, Medicaid, & does away with ACA, what will Trump's base feel about \"change\" then? That's a big one right?!\n",
      "true label = 0, predicted label = 1\n",
      "Swampbitch Nasty Pelosi  loves yelling 'Fire' in the crowded swamp. #blackfriday @user\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "\n",
    "\n",
    "def print_misclassified(\n",
    "    tweets: list[str],\n",
    "    labels: list[int],\n",
    "    pred_labels: numpy.typing.NDArray[Any],\n",
    "    n: int = 10,\n",
    "):\n",
    "    index = 0\n",
    "    for tweet, label, pred_label in zip(tweets, labels, pred_labels):\n",
    "        if index >= n:\n",
    "            break\n",
    "        if label != pred_label:\n",
    "            print(f\"true label = {label}, predicted label = {pred_label}\")\n",
    "            print(tweet)\n",
    "            index += 1\n",
    "\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_naive_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Logistic Regression Classifier\n",
    "\n",
    "**TODO 3.1:** Train a classifier using the [LogisticRegression class.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyright: reportUnknownMemberType=false\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(document_term_matrix.array, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3.2:** Obtain predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "\n",
    "pred_labels_logistic_regression = logistic_regression.predict(\n",
    "    document_term_matrix_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3.3:** Compute accuracy, precision, recall and F1 scores on the test set using [scikit-learn's metrics libary.](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label = 1, predicted label = 0\n",
      "@user @user what do these '1/2 naked pics' have to do with anything? They're not even like that.\n",
      "true label = 2, predicted label = 1\n",
      "I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
      "true label = 0, predicted label = 1\n",
      "@user Wow,first Hugo Chavez and now Fidel Castro. Danny Glover, Michael Moore, Oliver Stone, and Sean Penn are running out of heroes.\n",
      "true label = 1, predicted label = 0\n",
      "Savchenko now Saakashvili took drug test live on Ukraine TV. To prove they are not drug-fueled loonies?\n",
      "true label = 2, predicted label = 1\n",
      "Twitter's #ThankYouObama Shows Heartfelt Gratitude To POTUS\n",
      "true label = 1, predicted label = 2\n",
      "All CSG and Fracking all around Australia is to cease and Mining Entities held Accountable! - Sign the Petition!\n",
      "true label = 0, predicted label = 1\n",
      "@user @user @user @user @user @user take away illegals and dead people and Trump wins popular vote too.\n",
      "true label = 1, predicted label = 2\n",
      "An interesting security vulnerability - albeit not for the everyday car thief\n",
      "true label = 2, predicted label = 1\n",
      "#onedirection #harrystyles cute little dance 😉\n",
      "true label = 0, predicted label = 1\n",
      "When Ryan privatizes SS, Medicare, Medicaid, & does away with ACA, what will Trump's base feel about \"change\" then? That's a big one right?!\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO 3.3:** Print out the ten features with the highest weights for each class. Hint: use the `coef_` attribute of the LogisticRegression object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class = 0\n",
      "  fuck = 1.048\n",
      "  shit = 0.797\n",
      "  more = 0.785\n",
      "  something = 0.770\n",
      "  are = 0.736\n",
      "  hell = 0.730\n",
      "  muslims = 0.670\n",
      "  ira = 0.669\n",
      "  when = 0.665\n",
      "  worst = 0.661\n",
      "class = 1\n",
      "  from = 0.814\n",
      "  gonna = 0.634\n",
      "  set = 0.608\n",
      "  10 = 0.563\n",
      "  thought = 0.559\n",
      "  justin = 0.553\n",
      "  believe = 0.552\n",
      "  ps4 = 0.526\n",
      "  need = 0.525\n",
      "  sam = 0.524\n",
      "class = 2\n",
      "  happy = 1.225\n",
      "  good = 1.132\n",
      "  love = 1.072\n",
      "  best = 0.860\n",
      "  great = 0.858\n",
      "  birthday = 0.835\n",
      "  football = 0.828\n",
      "  excited = 0.751\n",
      "  another = 0.709\n",
      "  match = 0.693\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "\n",
    "\n",
    "def print_top_features_coef(\n",
    "    feature_names: numpy.typing.NDArray[Any],\n",
    "    classes: numpy.typing.NDArray[Any],\n",
    "    classes_coef: numpy.typing.NDArray[Any],\n",
    "    n: int = 10,\n",
    ") -> None:\n",
    "    for class_name, coefficients in zip(classes, classes_coef):\n",
    "        print(f\"class = {class_name}\")\n",
    "        features = sorted(\n",
    "            zip(feature_names, coefficients),\n",
    "            key=lambda feature: feature[1],\n",
    "            reverse=True,\n",
    "        )\n",
    "        for feature, log_prob in features[:n]:\n",
    "            print(f\"  {feature} = {log_prob:.3f}\")\n",
    "\n",
    "\n",
    "print_top_features_coef(\n",
    "    document_term_matrix.get_feature_names(),\n",
    "    logistic_regression.classes_,\n",
    "    logistic_regression.coef_,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3.4:** Print out an example of some misclassified tweets along with their predicted and true labels.\n",
    "\n",
    "**TODO 3.5:** What differences do you find between the results with NB and LR classifiers? Are there any kinds of common mistakes that either classifier makes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label = 1, predicted label = 0\n",
      "@user @user what do these '1/2 naked pics' have to do with anything? They're not even like that.\n",
      "true label = 2, predicted label = 1\n",
      "I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
      "true label = 0, predicted label = 1\n",
      "@user Wow,first Hugo Chavez and now Fidel Castro. Danny Glover, Michael Moore, Oliver Stone, and Sean Penn are running out of heroes.\n",
      "true label = 1, predicted label = 0\n",
      "Savchenko now Saakashvili took drug test live on Ukraine TV. To prove they are not drug-fueled loonies?\n",
      "true label = 2, predicted label = 1\n",
      "Twitter's #ThankYouObama Shows Heartfelt Gratitude To POTUS\n",
      "true label = 1, predicted label = 2\n",
      "All CSG and Fracking all around Australia is to cease and Mining Entities held Accountable! - Sign the Petition!\n",
      "true label = 0, predicted label = 1\n",
      "@user @user @user @user @user @user take away illegals and dead people and Trump wins popular vote too.\n",
      "true label = 1, predicted label = 2\n",
      "An interesting security vulnerability - albeit not for the everyday car thief\n",
      "true label = 2, predicted label = 1\n",
      "#onedirection #harrystyles cute little dance 😉\n",
      "true label = 0, predicted label = 1\n",
      "When Ryan privatizes SS, Medicare, Medicaid, & does away with ACA, what will Trump's base feel about \"change\" then? That's a big one right?!\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. N-grams and Lexicon Features\n",
    "\n",
    "We can try to improve the classifiers using some richer features.\n",
    "\n",
    "**TODO 4.1:** Use bigram features as well as unigrams (single tokens). To do these, change the `ngram_range` parameter in the CountVectorizer then try running the best classifier again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "\n",
    "from dn.embeddings.document_term_matrix import DocumentTermMatrix\n",
    "\n",
    "document_term_matrix_2 = DocumentTermMatrix(train_texts, ngram_range=(1, 2))\n",
    "document_term_matrix_2_test = document_term_matrix_2.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.456\n",
      "precision = 0.426\n",
      "recall = 0.421\n",
      "f1 = 0.341\n",
      "class = 0\n",
      "  ira = 0.739\n",
      "  french = 0.736\n",
      "  name of = 0.736\n",
      "  the ira = 0.736\n",
      "  the name = 0.736\n",
      "  user that = 0.736\n",
      "  wrong = 0.736\n",
      "  name = 0.702\n",
      "  marriage = 0.681\n",
      "  2nd try = 0.677\n",
      "class = 1\n",
      "  rollins = 0.780\n",
      "  seth rollins = 0.780\n",
      "  delhi = 0.759\n",
      "  plan = 0.759\n",
      "  dec = 0.702\n",
      "  gandhi = 0.702\n",
      "  seth = 0.692\n",
      "  sox = 0.691\n",
      "  january = 0.688\n",
      "  red sox = 0.688\n",
      "class = 2\n",
      "  happy = 0.914\n",
      "  birthday = 0.898\n",
      "  can wait = 0.816\n",
      "  amazing = 0.799\n",
      "  2nd birthday = 0.780\n",
      "  excited = 0.780\n",
      "  fun = 0.780\n",
      "  thank you = 0.780\n",
      "  love = 0.776\n",
      "  thank = 0.776\n",
      "true label = 1, predicted label = 2\n",
      "OH: “I had a blue penis while I was this” [playing with Google Earth VR]\n",
      "true label = 2, predicted label = 1\n",
      "I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
      "true label = 0, predicted label = 1\n",
      "@user Wow,first Hugo Chavez and now Fidel Castro. Danny Glover, Michael Moore, Oliver Stone, and Sean Penn are running out of heroes.\n",
      "true label = 1, predicted label = 2\n",
      "Savchenko now Saakashvili took drug test live on Ukraine TV. To prove they are not drug-fueled loonies?\n",
      "true label = 1, predicted label = 2\n",
      "How many more days until opening day? 😩\n",
      "true label = 2, predicted label = 1\n",
      "Twitter's #ThankYouObama Shows Heartfelt Gratitude To POTUS\n",
      "true label = 0, predicted label = 1\n",
      "@user @user @user @user @user @user take away illegals and dead people and Trump wins popular vote too.\n",
      "true label = 1, predicted label = 2\n",
      "An interesting security vulnerability - albeit not for the everyday car thief\n",
      "true label = 0, predicted label = 1\n",
      "When Ryan privatizes SS, Medicare, Medicaid, & does away with ACA, what will Trump's base feel about \"change\" then? That's a big one right?!\n",
      "true label = 0, predicted label = 1\n",
      "Swampbitch Nasty Pelosi  loves yelling 'Fire' in the crowded swamp. #blackfriday @user\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes_2 = MultinomialNB()\n",
    "naive_bayes_2.fit(document_term_matrix_2.array, train_labels)\n",
    "\n",
    "pred_labels_naive_bayes_2 = naive_bayes_2.predict(document_term_matrix_2_test)\n",
    "\n",
    "print_metrics(test_labels, pred_labels_naive_bayes_2)\n",
    "\n",
    "print_top_features_log_prob(\n",
    "    document_term_matrix_2.get_feature_names(),\n",
    "    naive_bayes_2.classes_,\n",
    "    naive_bayes_2.feature_log_prob_,\n",
    ")\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_naive_bayes_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.493\n",
      "precision = 0.450\n",
      "recall = 0.423\n",
      "f1 = 0.373\n",
      "class = 0\n",
      "  are = 0.606\n",
      "  fuck = 0.549\n",
      "  when = 0.538\n",
      "  more = 0.482\n",
      "  fuck it = 0.461\n",
      "  something = 0.445\n",
      "  so = 0.426\n",
      "  shit = 0.419\n",
      "  muslims = 0.412\n",
      "  get to = 0.406\n",
      "class = 1\n",
      "  from = 0.606\n",
      "  with = 0.439\n",
      "  thursday = 0.372\n",
      "  set = 0.368\n",
      "  be in = 0.363\n",
      "  10 = 0.343\n",
      "  gonna = 0.340\n",
      "  off = 0.329\n",
      "  said = 0.322\n",
      "  thought = 0.313\n",
      "class = 2\n",
      "  happy = 0.863\n",
      "  good = 0.823\n",
      "  love = 0.749\n",
      "  great = 0.620\n",
      "  best = 0.598\n",
      "  birthday = 0.539\n",
      "  friday = 0.464\n",
      "  win = 0.454\n",
      "  see = 0.450\n",
      "  excited = 0.444\n",
      "true label = 2, predicted label = 1\n",
      "I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
      "true label = 0, predicted label = 1\n",
      "@user Wow,first Hugo Chavez and now Fidel Castro. Danny Glover, Michael Moore, Oliver Stone, and Sean Penn are running out of heroes.\n",
      "true label = 1, predicted label = 0\n",
      "Savchenko now Saakashvili took drug test live on Ukraine TV. To prove they are not drug-fueled loonies?\n",
      "true label = 2, predicted label = 1\n",
      "Twitter's #ThankYouObama Shows Heartfelt Gratitude To POTUS\n",
      "true label = 1, predicted label = 2\n",
      "All CSG and Fracking all around Australia is to cease and Mining Entities held Accountable! - Sign the Petition!\n",
      "true label = 0, predicted label = 1\n",
      "@user @user @user @user @user @user take away illegals and dead people and Trump wins popular vote too.\n",
      "true label = 1, predicted label = 2\n",
      "An interesting security vulnerability - albeit not for the everyday car thief\n",
      "true label = 2, predicted label = 1\n",
      "#onedirection #harrystyles cute little dance 😉\n",
      "true label = 0, predicted label = 1\n",
      "When Ryan privatizes SS, Medicare, Medicaid, & does away with ACA, what will Trump's base feel about \"change\" then? That's a big one right?!\n",
      "true label = 0, predicted label = 1\n",
      "Swampbitch Nasty Pelosi  loves yelling 'Fire' in the crowded swamp. #blackfriday @user\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression_2 = LogisticRegression()\n",
    "logistic_regression_2.fit(document_term_matrix_2.array, train_labels)\n",
    "\n",
    "pred_labels_logistic_regression_2 = logistic_regression_2.predict(\n",
    "    document_term_matrix_2_test\n",
    ")\n",
    "\n",
    "print_metrics(test_labels, pred_labels_logistic_regression_2)\n",
    "\n",
    "print_top_features_coef(\n",
    "    document_term_matrix_2.get_feature_names(),\n",
    "    logistic_regression_2.classes_,\n",
    "    logistic_regression_2.coef_,\n",
    ")\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_logistic_regression_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "For sentiment analysis, we can also make use of lexicons. Lexicons are lists of words associated with a particular property, such as positive sentiment. Because these lists were constructed in advance, we don't need to learn the associations between words and sentiment classes purely from the training data. This is useful because some words may be present in the test data but occur rarely, or never at all, in the training set.\n",
    "\n",
    "Here is one way we can use a lexicon to create some new features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/qr23940/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportMissingTypeStubs=false\n",
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "vocabulary = document_term_matrix.get_feature_names()\n",
    "\n",
    "lex_pos_scores = np.zeros((1, len(vocabulary)))\n",
    "lex_neg_scores = np.zeros((1, len(vocabulary)))\n",
    "\n",
    "for index, term in enumerate(vocabulary):\n",
    "    if term in analyser.lexicon and analyser.lexicon[term] > 0:\n",
    "        lex_pos_scores[0, index] = 1\n",
    "    elif term in analyser.lexicon and analyser.lexicon[term] < 0:\n",
    "        lex_neg_scores[0, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max positive train = 7.0\n",
      "max positive test = 4.0\n",
      "max negative train = 4.0\n",
      "max negative test = 4.0\n",
      "(1000, 4665)\n",
      "(1000, 4665)\n",
      "(1000, 1)\n",
      "(1000, 1)\n",
      "(1000, 1)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportMissingTypeStubs=false\n",
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "\n",
    "import numpy\n",
    "\n",
    "lex_pos_train = numpy.sum(\n",
    "    numpy.multiply(document_term_matrix.array, lex_pos_scores), axis=1\n",
    ")\n",
    "\n",
    "print(f\"max positive train = {numpy.max(lex_pos_train)}\")\n",
    "\n",
    "lex_pos_test = numpy.sum(\n",
    "    numpy.multiply(document_term_matrix_test, lex_pos_scores), axis=1\n",
    ")\n",
    "\n",
    "print(f\"max positive test = {numpy.max(lex_pos_test)}\")\n",
    "\n",
    "lex_neg_train = numpy.sum(\n",
    "    numpy.multiply(document_term_matrix.array, lex_neg_scores), axis=1\n",
    ")\n",
    "\n",
    "print(f\"max negative train = {numpy.max(lex_neg_train)}\")\n",
    "\n",
    "lex_neg_test = numpy.sum(\n",
    "    numpy.multiply(document_term_matrix_test, lex_neg_scores), axis=1\n",
    ")\n",
    "\n",
    "print(f\"max negative test = {numpy.max(lex_neg_test)}\")\n",
    "\n",
    "# TODO: fix this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can append the counts to the feature vector and treat them as extra features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1000,4665) into shape (1000,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/qr23940/git/dialogue_and_narrative/src/notebooks/3_classifiers.ipynb Cell 35\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qr23940/git/dialogue_and_narrative/src/notebooks/3_classifiers.ipynb#Y130sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# pyright: reportMissingTypeStubs=false\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qr23940/git/dialogue_and_narrative/src/notebooks/3_classifiers.ipynb#Y130sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# pyright: reportUnknownVariableType=false\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qr23940/git/dialogue_and_narrative/src/notebooks/3_classifiers.ipynb#Y130sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m hstack\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/qr23940/git/dialogue_and_narrative/src/notebooks/3_classifiers.ipynb#Y130sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m lex_train \u001b[39m=\u001b[39m hstack((document_term_matrix\u001b[39m.\u001b[39marray, lex_pos_train, lex_neg_train))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/qr23940/git/dialogue_and_narrative/src/notebooks/3_classifiers.ipynb#Y130sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m lex_test \u001b[39m=\u001b[39m hstack((document_term_matrix_test, lex_pos_test, lex_neg_test))\n",
      "File \u001b[0;32m~/miniconda3/envs/dn/lib/python3.11/site-packages/scipy/sparse/_construct.py:535\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhstack\u001b[39m(blocks, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    506\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[39m    Stack sparse matrices horizontally (column wise)\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    533\u001b[0m \n\u001b[1;32m    534\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m bmat([blocks], \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mformat\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/dn/lib/python3.11/site-packages/scipy/sparse/_construct.py:615\u001b[0m, in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbmat\u001b[39m(blocks, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    573\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[39m    Build a sparse matrix from sparse sub-blocks\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m \n\u001b[1;32m    613\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m     blocks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(blocks, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    617\u001b[0m     \u001b[39mif\u001b[39;00m blocks\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    618\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mblocks must be 2-D\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (1000,4665) into shape (1000,)"
     ]
    }
   ],
   "source": [
    "# pyright: reportMissingTypeStubs=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "lex_train = hstack((document_term_matrix.array, lex_pos_train, lex_neg_train))\n",
    "lex_test = hstack((document_term_matrix_test, lex_pos_test, lex_neg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 4.2:** Use the new X_train and X_test feature vectors to train and evaluate your classifier.\n",
    "Does adding the lexicon features improve performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes_3 = MultinomialNB()\n",
    "naive_bayes_3.fit(lex_train, train_labels)\n",
    "\n",
    "pred_labels_naive_bayes_3 = naive_bayes_3.predict(lex_test)\n",
    "\n",
    "print_metrics(test_labels, pred_labels_naive_bayes_3)\n",
    "\n",
    "print_top_features_log_prob(\n",
    "    document_term_matrix.get_feature_names(),\n",
    "    naive_bayes_3.classes_,\n",
    "    naive_bayes_3.feature_log_prob_,\n",
    ")\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_naive_bayes_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression_3 = LogisticRegression()\n",
    "logistic_regression_3.fit(lex_train, train_labels)\n",
    "\n",
    "pred_labels_logistic_regression_3 = logistic_regression_3.predict(lex_test)\n",
    "\n",
    "print_metrics(test_labels, pred_labels_logistic_regression_3)\n",
    "\n",
    "print_top_features_coef(\n",
    "    document_term_matrix.get_feature_names(),\n",
    "    logistic_regression_3.classes_,\n",
    "    logistic_regression_3.coef_,\n",
    ")\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_logistic_regression_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialogue_and_narrative",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
