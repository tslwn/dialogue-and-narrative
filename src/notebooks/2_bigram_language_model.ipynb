{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n",
    "\n",
    "In this lab we will implement a bigram language model and use it to compute the probability of some sample sentences.\n",
    "\n",
    "As you go through, make sure you understand what's going on in each cell, and ask if it is unclear.\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "- Know how to count word frequencies in a corpus using Python libraries.\n",
    "- Understand how to compute conditional probabilities.\n",
    "- Be able to apply the chain rule to compute the probability of a sentence.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads the same dataset as last week.\n",
    "The next part splits the data into training and test sets, and tokenises the utterances.\n",
    "After this there are some tasks to complete to implement and test the language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dn.doc2dial import load_dataset\n",
    "\n",
    "docs = load_dataset(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Can', 'I', 'do', 'my', 'DMV', 'transactions', 'online', '?', '</s>']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from dn.bigram_language_model.preprocessing import pad\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenize the utterances.\n",
    "docs = [[token.text for token in nlp(doc)] for doc in docs]\n",
    "\n",
    "# Pad the utterances.\n",
    "docs = [list(pad(doc)) for doc in docs]\n",
    "\n",
    "# Print an example of a tokenized utterance.\n",
    "print(docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size = 956\n",
      "test size = 239\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train: list[list[str]]\n",
    "test: list[list[str]]\n",
    "\n",
    "# Split the data into training and test sets with `scikit-learn`.\n",
    "train, test = train_test_split(docs, train_size=0.8, test_size=0.2)\n",
    "\n",
    "print(f\"train size = {len(train)}\")\n",
    "print(f\"test size = {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Counting Tokens\n",
    "\n",
    "The bigram language model needs to compute two sets of counts from the training data:\n",
    "\n",
    "1. The counts of how many times each bigram occurs.\n",
    "2. The counts of how many times each word type occurs as the first token in a bigram.\n",
    "\n",
    "Let's start by finding the vocabulary of unique token 'types':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size = 1680\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "vocab = list(numpy.unique(numpy.concatenate(train)).tolist())\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"vocab size = {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "# For example, we can find the index of a token like so:\n",
    "def find_token_index(token: str, vocab: list[str]):\n",
    "    try:\n",
    "        return vocab.index(token)\n",
    "        # If `vocab` were a numpy array, we could find the token's index like so:\n",
    "        # `return np.argwhere(vocab == token)[0][0]`\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "print(find_token_index(\"<s>\", vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "# Define a function to print the bigram statistics of a tokenized utterance.\n",
    "def print_bigram_statistics(bigram_matrix: NDArray, tokenized: list[str]):\n",
    "    bigram_statistics = []\n",
    "    for token_current, token_next in zip(\n",
    "        tokenized,\n",
    "        tokenized[1:],\n",
    "    ):\n",
    "        # Find the indices of the tokens in the vocabulary.\n",
    "        token_current_index = find_token_index(token_current, vocab)\n",
    "        token_next_index = find_token_index(token_next, vocab)\n",
    "\n",
    "        # If both tokens are in the vocabulary, find the value of the statistic\n",
    "        # for the bigram and add it to the list to print.\n",
    "        if token_current_index != -1 and token_next_index != -1:\n",
    "            bigram_statistic = bigram_matrix[\n",
    "                token_current_index, token_next_index\n",
    "            ]\n",
    "            bigram_statistics.append(\n",
    "                f\"{token_current} {token_next}: {bigram_statistic}\"\n",
    "            )\n",
    "\n",
    "    pprint(bigram_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.1:** count the bigrams that occur in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> Can: 39.0',\n",
      " 'Can I: 24.0',\n",
      " 'I do: 40.0',\n",
      " 'do my: 4.0',\n",
      " 'my DMV: 4.0',\n",
      " 'DMV transactions: 3.0',\n",
      " 'transactions online: 2.0',\n",
      " 'online ?: 7.0',\n",
      " '? </s>: 378.0']\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "def find_bigram_counts(utterances: list[list[str]], vocab_size: int):\n",
    "    # A matrix whose row indices correspond to the first tokens in bigrams and\n",
    "    # column indices correspond to the second tokens in bigrams. The indices must\n",
    "    # map to the index of the token in the vocabulary. The values of the matrix will\n",
    "    # be the token counts. We initialize the matrix with ones to use add-one\n",
    "    # smoothing.\n",
    "    bigram_counts = numpy.ones((vocab_size, vocab_size))\n",
    "\n",
    "    tokens = [token for utterance in utterances for token in utterance]\n",
    "\n",
    "    # Iterate the tokens in each utterance pairwise.\n",
    "    for token_current, token_next in zip(tokens, tokens[1:]):\n",
    "        # Find the indices of the tokens in the vocabulary.\n",
    "        token_current_index = find_token_index(token_current, vocab)\n",
    "        token_next_index = find_token_index(token_next, vocab)\n",
    "\n",
    "        # If both tokens are in the vocabulary, increment the bigram count.\n",
    "        if token_current_index != -1 and token_next_index != -1:\n",
    "            bigram_counts[token_current_index, token_next_index] += 1\n",
    "\n",
    "    return bigram_counts\n",
    "\n",
    "\n",
    "bigram_counts = find_bigram_counts(train, vocab_size)\n",
    "\n",
    "# Print the counts of the bigrams in an example of a tokenized utterance.\n",
    "print_bigram_statistics(bigram_counts, docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.2:** Apply numpy's sum() function to the 'counts' variable to compute the number of times each word type occurs as the first token in a bigram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigram first-token counts:\n",
      "['<s>: 2636.0',\n",
      " 'Can: 1725.0',\n",
      " 'I: 2097.0',\n",
      " 'do: 1833.0',\n",
      " 'my: 1897.0',\n",
      " 'DMV: 1847.0',\n",
      " 'transactions: 1688.0',\n",
      " 'online: 1714.0',\n",
      " '?: 2080.0',\n",
      " '</s>: 2635.0']\n",
      "\n",
      "Unigram counts:\n",
      "['<s>: 956.0',\n",
      " 'Can: 45.0',\n",
      " 'I: 417.0',\n",
      " 'do: 153.0',\n",
      " 'my: 217.0',\n",
      " 'DMV: 167.0',\n",
      " 'transactions: 8.0',\n",
      " 'online: 34.0',\n",
      " '?: 400.0',\n",
      " '</s>: 956.0']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import numpy\n",
    "\n",
    "# `axis=1`: apply the operation row-wise, i.e., across all rows for each column.\n",
    "first_token_counts = bigram_counts.sum(axis=1)\n",
    "\n",
    "\n",
    "# Compute the unigram counts. The result should be the same.\n",
    "def find_unigram_counts(utterances: list[list[str]], vocab_size: int):\n",
    "    # A vector whose indices correspond to the tokens in the vocabulary.\n",
    "    unigram_counts = numpy.zeros(vocab_size)\n",
    "\n",
    "    # Iterate the tokens in the utterances.\n",
    "    for token in [token for utterance in utterances for token in utterance]:\n",
    "        # Find the index of the token in the vocabulary.\n",
    "        token_index = find_token_index(token, vocab)\n",
    "\n",
    "        # If the token is in the vocabulary, increment the unigram count.\n",
    "        if token_index != -1:\n",
    "            unigram_counts[token_index] += 1\n",
    "\n",
    "    return unigram_counts\n",
    "\n",
    "\n",
    "unigram_counts = find_unigram_counts(train, vocab_size)\n",
    "\n",
    "# Print the counts with which the tokens in an example of a tokenized utterance\n",
    "# occur as the first token in a bigram.\n",
    "print(\"\\nBigram first-token counts:\")\n",
    "pprint(\n",
    "    [\n",
    "        f\"{token}: {first_token_counts[find_token_index(token, vocab)]}\"\n",
    "        for token in docs[2]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the unigram counts, which are smaller than the bigram first-token counts\n",
    "# by the size of the vocabulary.\n",
    "print(\"\\nUnigram counts:\")\n",
    "pprint(\n",
    "    [\n",
    "        f\"{token}: {unigram_counts[find_token_index(token, vocab)]}\"\n",
    "        for token in docs[2]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.3:** Compute a matrix (numpy array) of conditional probabilities using the counts. Compute the log of this matrix as a variable 'log_cond_probs'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our results:\n",
      "\n",
      "Bigram conditional probabilities:\n",
      "['<s> Can: 0.022608695652173914',\n",
      " 'Can I: 0.011444921316165951',\n",
      " 'I do: 0.02182214948172395',\n",
      " 'do my: 0.0021085925144965737',\n",
      " 'my DMV: 0.002165674066053059',\n",
      " 'DMV transactions: 0.0017772511848341231',\n",
      " 'transactions online: 0.0011668611435239206',\n",
      " 'online ?: 0.0033653846153846156',\n",
      " '? </s>: 0.14345351043643265']\n",
      "\n",
      "Logarithms of bigram conditional probabilities:\n",
      "['<s> Can: -3.7894206833358135',\n",
      " 'Can I: -4.470209200553974',\n",
      " 'I do: -3.8248297937257685',\n",
      " 'do my: -6.161734608815124',\n",
      " 'my DMV: -6.135023619079349',\n",
      " 'DMV transactions: -6.332687386487793',\n",
      " 'transactions online: -6.75343791859778',\n",
      " 'online ?: -5.69421302364005',\n",
      " '? </s>: -1.941744265355875']\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from numpy.typing import NDArray\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# Define a function to print the bigram statistics of a tokenized utterance.\n",
    "def print_bigram_statistics(bigram_matrix: NDArray, tokenized: list[str]):\n",
    "    bigram_statistics = []\n",
    "    for token_current, token_next in zip(\n",
    "        tokenized,\n",
    "        tokenized[1:],\n",
    "    ):\n",
    "        # Find the indices of the tokens in the vocabulary.\n",
    "        token_current_index = find_token_index(token_current, vocab)\n",
    "        token_next_index = find_token_index(token_next, vocab)\n",
    "\n",
    "        # If both tokens are in the vocabulary, find the value of the statistic\n",
    "        # for the bigram and add it to the list to print.\n",
    "        if token_current_index != -1 and token_next_index != -1:\n",
    "            bigram_statistic = bigram_matrix[\n",
    "                token_current_index, token_next_index\n",
    "            ]\n",
    "            bigram_statistics.append(\n",
    "                f\"{token_current} {token_next}: {bigram_statistic}\"\n",
    "            )\n",
    "\n",
    "    pprint(bigram_statistics)\n",
    "\n",
    "\n",
    "# Compute the bigram conditional probabilities.\n",
    "bigram_conditional_probabilities = numpy.divide(\n",
    "    bigram_counts, first_token_counts\n",
    ")\n",
    "\n",
    "print(\"\\nBigram conditional probabilities:\")\n",
    "print_bigram_statistics(bigram_conditional_probabilities, docs[2])\n",
    "\n",
    "# Compute the logarithms of the bigram conditional probabilities.\n",
    "bigram_conditional_log_probabilities = numpy.log(\n",
    "    bigram_conditional_probabilities\n",
    ")\n",
    "\n",
    "print(\"\\nLogarithms of bigram conditional probabilities:\")\n",
    "print_bigram_statistics(bigram_conditional_log_probabilities, docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.4:** Write a function that uses log_cond_probs to compute the probability of a given tokenised sentence, such as the example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a tokenized utterance.\n",
    "doc_example = [\n",
    "    \"<s>\",\n",
    "    \"If\",\n",
    "    \"you\",\n",
    "    \"give\",\n",
    "    \"me\",\n",
    "    \"the\",\n",
    "    \"help\",\n",
    "    \",\",\n",
    "    \"what\",\n",
    "    \"is\",\n",
    "    \"the\",\n",
    "    \"payment\",\n",
    "    \"system\",\n",
    "    \"?\",\n",
    "    \"<e>\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2375525734514128e-35\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def find_log_probability(\n",
    "    tokenized: list[str],\n",
    "    vocab: list[str],\n",
    "    bigram_matrix: NDArray,\n",
    "):\n",
    "    log_probability = 0.0\n",
    "    for token_current, token_next in zip(\n",
    "        tokenized,\n",
    "        tokenized[1:],\n",
    "    ):\n",
    "        # Find the indices of the tokens in the vocabulary.\n",
    "        token_current_index = find_token_index(token_current, vocab)\n",
    "        token_next_index = find_token_index(token_next, vocab)\n",
    "\n",
    "        # If both tokens are in the vocabulary, add to the log probability.\n",
    "        if token_current_index != -1 and token_next_index != -1:\n",
    "            log_probability += bigram_matrix[\n",
    "                token_current_index, token_next_index\n",
    "            ]\n",
    "\n",
    "    return log_probability\n",
    "\n",
    "\n",
    "pprint(\n",
    "    np.exp(\n",
    "        find_log_probability(\n",
    "            doc_example, vocab, bigram_conditional_log_probabilities\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.5:** Compute the perplexity over the whole test set. You will need to make sure your code can handle unknown words -- make sure it does not end up misusing the index of -1 returned by get_index_for_word() for unknown words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466.51348396537867\n"
     ]
    }
   ],
   "source": [
    "from numpy.typing import NDArray\n",
    "import math\n",
    "from dn.bigram_language_model.preprocessing import flatten\n",
    "\n",
    "\n",
    "def find_perplexity(\n",
    "    tokenized: list[str],\n",
    "    vocab: list[str],\n",
    "    bigram_matrix: NDArray,\n",
    "):\n",
    "    \"\"\"\n",
    "    The perplexity is the Nth root of the product of the inverse probabilities\n",
    "    of the bigrams, where N is the number of bigrams. Because of the properties\n",
    "    of logarithms, this is equivalent to (pseudocode):\n",
    "    ```\n",
    "        exp(1 - sum(log(probability(bigram)))/N)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    return math.exp(\n",
    "        1\n",
    "        - find_log_probability(tokenized, vocab, bigram_matrix)\n",
    "        / (len(tokenized) - 1)\n",
    "    )\n",
    "\n",
    "\n",
    "# Flatten the test set into a single \"document\".\n",
    "test_flat = list(flatten(test))\n",
    "\n",
    "pprint(find_perplexity(test_flat, vocab, bigram_conditional_log_probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENSION 1:** Use the language model to generate new sentences by sampling.\n",
    "You can follow the example below to sample using scipy's multinomial class. Replace the distribution with the conditional distribution we computed earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MORE EXTENSIONS:\n",
    "\n",
    "- Add some smoothing to the counts and see how it affects the results.\n",
    "- Use trigrams instead of bigrams. Does it improve perplexity?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialogue_and_narrative",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
