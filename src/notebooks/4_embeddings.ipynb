{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "In this lab we will use both sparse vectors and dense word2vec embeddings to obtain vector representations of words and documents.\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "- Be able to compute term-document matrices from a collection of text documents.\n",
    "- Be able to implement cosine similarity.\n",
    "- Know how to use Gensim to train, download and apply word embedding models.\n",
    "- Understand the word analogy task for word embeddings.\n",
    "\n",
    "### Overview\n",
    "\n",
    "First, we will load another set of tweet data. Then, we will obtain a term-document matrix, and compute cosine similarities. Then, we will use the Gensim library to train a word2vec model and download a pretrained model. Finally, we use the Gensim embeddings to perform the analogy task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data\n",
    "\n",
    "Instead of the sentiment classification dataset, we will work with the smaller emotion classification dataset. The dataset labels tweets as one of the following classes:\n",
    "\n",
    "- 0: anger\n",
    "- 1: joy\n",
    "- 2: optimism\n",
    "- 3: sadness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tweet_eval (/Users/qr23940/git/dialogue_and_narrative/src/notebooks/data_cache/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    }
   ],
   "source": [
    "from dn.datasets import TweetEvalDataset\n",
    "\n",
    "train = TweetEvalDataset(\"emotion\", \"train\")\n",
    "test = TweetEvalDataset(\"emotion\", \"test\")\n",
    "\n",
    "train_texts: list[str] = []\n",
    "train_labels: list[int] = []\n",
    "\n",
    "for item in train.iter():\n",
    "    train_texts.append(item[\"text\"])\n",
    "    train_labels.append(item[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Term-Document Matrix\n",
    "\n",
    "**TO-DO 1.1:** Use the CountVectorizer, as in week 3, to obtain a term-document matrix for the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dn.embeddings.document_term_matrix import DocumentTermMatrix\n",
    "\n",
    "document_term_matrix = DocumentTermMatrix(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 1.2:** Print out the term vector for the word 'happy'. Use the vocabulary\\_ attribute to look up the word's index.\n",
    "_Hint:_ the CountVectorizer stores a term-document matrix in a sparse format to save memory. You can convert this to a standard numpy array using the method '.toarray()'.\n",
    "_Hint:_ you can use the method '.flatten()' to convert a 1xN matrix to a vector.\n",
    "\n",
    "The print-out probably won't be terribly readable, so you will need to convince yourself you have obtained the correct vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term index = 3295\n",
      "Term vector = [0 0 0 ... 0 0 0]\n",
      "Term documents = \n",
      "\tHappy birthday to Stephen King, a man responsible for some of the best horror of the past 40 years... and a whole bunch of the worst.\n",
      "\tHappy Birthday, LOST! / #lost #dharmainitiative #12years #22september2004 #oceanic815\n",
      "\t#PeopleLikeMeBecause they see the happy exterior, not the hopelessness I sometimes feel inside. #depression  #anxietyprobz\n",
      "\t#PeopleLikeMeBecause they see the happy exterior, not the hopelessness I sometimes feel inside. #depression #anxiety #anxietyprobz\n",
      "\tHappy Birthday @user #cheer #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer\n",
      "\t@user people have so much negativity filled inside them but im always happy that in such a gloomy world someone like u exists Namjoon\n",
      "\tWell stock finished &amp; listed, living room moved around, new editing done &amp; fitted in a visit to the in-laws. #productivityatitsfinest #happy\n",
      "\t@user Thank you, happy birthday to you as well!\n",
      "\tHappy Birthday @user  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer\n",
      "\tNever make a #decision when you're #angry and never make a #promise when you're #happy. #wisewords\n"
     ]
    }
   ],
   "source": [
    "print(f\"Term index = {document_term_matrix.term_index('happy')}\")\n",
    "print(f\"Term vector = {document_term_matrix.term_vector('happy')}\")\n",
    "print(f\"Term documents = \")\n",
    "for document in document_term_matrix.term_documents(\"happy\", 10):\n",
    "    print(f\"\\t{document}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 1.3:** Print out the document vector for the first tweet in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document vector = [0 0 0 ... 0 0 0]\n",
      "Document terms = \n",
      "\tdown\n",
      "\thave\n",
      "\tis\n",
      "\tjoyce\n",
      "\tleadership\n",
      "\tmay\n",
      "\tmeyer\n",
      "\tmotivation\n",
      "\tnever\n",
      "\ton\n",
      "\tpayment\n",
      "\tproblem\n",
      "\tworry\n",
      "\tyou\n"
     ]
    }
   ],
   "source": [
    "print(f\"Document vector = {document_term_matrix.document_vector(0)}\")\n",
    "print(f\"Document terms = \")\n",
    "\n",
    "for term in document_term_matrix.document_terms(0):\n",
    "    print(f\"\\t{term}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cosine Similarity\n",
    "\n",
    "**TO-DO 2.1:** Write a function that computes cosine similarity between two vectors. _Hint:_ you might find numpy's linalg library useful. Refer to the textbook for a definition of cosine similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.2:** Use the function to find the five most similar words to 'happy' according to the document-term matrix. _Hint:_ the vocab_inverted dictionary that we compute below lets you look up a word given its index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birthday = 0.400\n",
      "ampalaya = 0.280\n",
      "paitpaitanangpeg = 0.280\n",
      "exterior = 0.243\n",
      "hopelessness = 0.243\n"
     ]
    }
   ],
   "source": [
    "for term, similarity in document_term_matrix.most_similar(\"happy\", 5):\n",
    "    print(f\"{term} = {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word2Vec\n",
    "\n",
    "For this part, we will need the gensim library. The code below tokenizes the training texts, then runs word2vec (the skipgram model) to learn a set of embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dn.embeddings.gensim_word2vec import GensimWord2Vec\n",
    "\n",
    "gensim_word2vec = GensimWord2Vec(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look up the embedding for any given word like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term vector = [ 0.093519   -0.08279113  0.3580625  -0.55562764  0.10134792 -0.26659063\n",
      "  0.31592056  0.52575016 -0.60725075 -0.4336265  -0.13637583 -0.01111981\n",
      "  0.17245735  0.2303386   0.1331783   0.82918245  0.4227791   0.5091341\n",
      " -0.46667728  0.3583106   0.46896124  0.51394933  0.41899118 -0.06921134\n",
      "  0.7425835 ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Term vector = {gensim_word2vec.term_vector('happy')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 3.1:** Now, use your cosine similarity method again to find the five most similar words to 'happy' according to your word2vec model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shocking = 0.997\n",
      "take = 0.997\n",
      "taking = 0.997\n",
      "horrid = 0.997\n",
      "though = 0.997\n"
     ]
    }
   ],
   "source": [
    "for term, similarity in gensim_word2vec.most_similar(\"happy\", 5):\n",
    "    print(f\"{term} = {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO 3.2:** Have either of these embeddings been effective at finding similar words? What might improve them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Downloading Pretrained Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Above, we trained our own model using the skipgram method. We can also download a pretrained model that has previously been trained on a large corpus. There is a list of models available [here](https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models). Let's try out GLoVe embeddings (another way of learning embeddings than using the skipgram model) trained on a corpus of tweets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term vector = [-1.2304   0.48312  0.14102 -0.0295  -0.65253 -0.18554  2.1033   1.7516\n",
      " -1.3001  -0.32113 -0.84774  0.41995 -3.8823   0.19638 -0.72865 -0.85273\n",
      "  0.23174 -1.0763  -0.83023  0.10815 -0.51015  0.27691 -1.1895   0.98094\n",
      " -0.13955]\n"
     ]
    }
   ],
   "source": [
    "from dn.embeddings.gensim_glove import GensimGlove\n",
    "\n",
    "gensim_glove = GensimGlove(train_texts)\n",
    "\n",
    "print(f\"Term vector = {gensim_glove.term_vector('happy')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 4.1:** Repeat the exercise above to find the closest relations to 'happy' with the downloaded model. How do the results compare to the embeddings we trained ourselves?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birthday = 0.958\n",
      "thank = 0.938\n",
      "welcome = 0.934\n",
      "love = 0.918\n",
      "miss = 0.916\n"
     ]
    }
   ],
   "source": [
    "for term, similarity in gensim_glove.most_similar(\"happy\", 5):\n",
    "    print(f\"{term} = {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Analogy Task\n",
    "\n",
    "An analogy can be formalised as:\n",
    "\n",
    "A is to B as A* is to B*.\n",
    "\n",
    "The analogy task is to find B* given A, B and A*.\n",
    "\n",
    "**TO-DO 5.1:** Define a function that can find the top N closest words B* for any given A, B and A*, using the Gensim embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sourcing', 0.8338578343391418)\n",
      "('traveller', 0.8320027589797974)\n",
      "('programmer', 0.8229867815971375)\n",
      "('marketer', 0.8201889991760254)\n",
      "('optimization', 0.8200806975364685)\n",
      "('multilingual', 0.8142974972724915)\n",
      "('caregiver', 0.8140866160392761)\n",
      "('xstrology', 0.8124445080757141)\n",
      "('telesales', 0.8106080889701843)\n",
      "('columnists', 0.8105406165122986)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "def most_similar_analogies(\n",
    "    term1: str,\n",
    "    term2: str,\n",
    "    term3: str,\n",
    "    term_vectors: KeyedVectors,\n",
    "    topn: int,\n",
    ") -> list[tuple[str, float]]:\n",
    "    term_vector = (\n",
    "        term_vectors[term3] + term_vectors[term2] - term_vectors[term1]\n",
    "    )\n",
    "    return term_vectors.similar_by_vector(term_vector, topn=topn)\n",
    "\n",
    "\n",
    "for analogy in most_similar_analogies(\n",
    "    \"man\", \"programmer\", \"woman\", gensim_glove.vectors, 10\n",
    "):\n",
    "    print(analogy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialogue_and_narrative",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
