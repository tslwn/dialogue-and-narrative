{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "This lab explores a new dataset for text classification tasks using naïve Bayes and logistic regression.\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "- Train and test naive_bayes and LR classifiers using an established library.\n",
    "- Apply evaluation metrics to the classifiers and display examples of misclassifications.\n",
    "- Examine learned model parameters to explain how each classifier makes a decision.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads a new Twitter dataset, which is described in [this paper](https://arxiv.org/pdf/2010.12421.pdf), then extracts feature vectors from each sample.\n",
    "The next part involves implementing and evaluating the classifiers using Scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tweet_eval (/Users/qr23940/Documents/git/dialogue_and_narrative/notebooks/data_cache/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "Found cached dataset tweet_eval (/Users/qr23940/Documents/git/dialogue_and_narrative/notebooks/data_cache/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    }
   ],
   "source": [
    "from Modules.datasets import TweetEvalDataset\n",
    "\n",
    "train = TweetEvalDataset(\"sentiment\", \"train\")\n",
    "test = TweetEvalDataset(\"sentiment\", \"test\")\n",
    "\n",
    "train_texts: list[str] = []\n",
    "train_labels: list[int] = []\n",
    "\n",
    "for item in train.iter():\n",
    "    train_texts.append(item[\"text\"])\n",
    "    train_labels.append(item[\"label\"])\n",
    "\n",
    "test_texts: list[str] = []\n",
    "test_labels: list[int] = []\n",
    "\n",
    "for item in test.iter():\n",
    "    test_texts.append(item[\"text\"])\n",
    "    test_labels.append(item[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert the tokenised text of each tweet to a feature vectors that we can use as input to a classifier. The feature vector needs to be a numerical vector of a fixed size. For the bag-of-words representation, the feature vector for a tweet will represent the number of occurrences of each word in the vocabulary in that tweet.\n",
    "\n",
    "For this, we can use the CountVectorizer class: [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "**TO DO 1.1:** Why do we need to fit the CountVectorizer on the train set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from Modules.term_vectors.document_term_matrix import DocumentTermMatrix\n",
    "\n",
    "document_term_matrix = DocumentTermMatrix(train_texts)\n",
    "document_term_matrix_test = document_term_matrix.transform(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Naive Bayes Classifier\n",
    "\n",
    "The code above has obtained the feature vectors and lists of labels. The data is now ready for use\n",
    "with scikit-learn's classifiers.\n",
    "\n",
    "**TODO 2.1:** Train a classifier using the [MultinomialNB class.](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) You will need to look at the linked documentation to see how to construct and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyright: reportUnknownMemberType=false\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(document_term_matrix.matrix, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO 2.2:** Again use the documentation to write code to obtain predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "\n",
    "pred_labels_naive_bayes = naive_bayes.predict(document_term_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO 2.3:** Compute accuracy, precision, recall and F1 scores on the test set using [scikit-learn's metrics libary.](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules) Review the documentation to see the different options for evaluating classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.581\n",
      "precision = 0.570\n",
      "recall = 0.588\n",
      "f1 = 0.576\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "\n",
    "from typing import Any, Literal\n",
    "import numpy.typing\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "Average = Literal[\"micro\", \"macro\", \"samples\", \"weighted\", \"binary\"] | None\n",
    "\n",
    "\n",
    "def print_metrics(\n",
    "    test_labels: list[int],\n",
    "    pred_labels: numpy.typing.NDArray[Any],\n",
    "    average: Average = \"macro\",\n",
    "):\n",
    "    accuracy = accuracy_score(test_labels, pred_labels)\n",
    "    print(f\"accuracy = {accuracy:.3f}\")\n",
    "    precision = precision_score(test_labels, pred_labels, average=average)\n",
    "    print(f\"precision = {precision:.3f}\")\n",
    "    recall = recall_score(test_labels, pred_labels, average=average)\n",
    "    print(f\"recall = {recall:.3f}\")\n",
    "    f1 = f1_score(test_labels, pred_labels, average=average)\n",
    "    print(f\"f1 = {f1:.3f}\")\n",
    "\n",
    "\n",
    "print_metrics(test_labels, pred_labels_naive_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO 2.4:** Print out the ten features with the strongest association with each class. Hint: use the `feature_log_prob_` attribute of the MultinomialNB object. You may also need Numpy's argsort() function.\n",
    "\n",
    "Beware offensive words below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class = 0\n",
      "  insult = 0.923\n",
      "  militants = 0.923\n",
      "  disgusting = 0.917\n",
      "  blah = 0.914\n",
      "  asshole = 0.912\n",
      "  idiots = 0.912\n",
      "  cunt = 0.897\n",
      "  graduated = 0.897\n",
      "  moron = 0.891\n",
      "  pathetic = 0.888\n",
      "class = 1\n",
      "  betis = 0.813\n",
      "  zap = 0.812\n",
      "  departure = 0.805\n",
      "  lucie = 0.805\n",
      "  exo = 0.800\n",
      "  obi = 0.790\n",
      "  thine = 0.776\n",
      "  tiffany = 0.776\n",
      "  moderate = 0.765\n",
      "  pacquiao = 0.765\n",
      "class = 2\n",
      "  amazing = 0.930\n",
      "  congrats = 0.905\n",
      "  excited = 0.892\n",
      "  blessed = 0.886\n",
      "  awesome = 0.885\n",
      "  exciting = 0.874\n",
      "  happy = 0.870\n",
      "  delicious = 0.868\n",
      "  enjoyed = 0.857\n",
      "  congratulations = 0.856\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "\n",
    "import numpy\n",
    "import numpy.typing\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def print_top_features_log_prob(\n",
    "    feature_names: numpy.typing.NDArray[Any],\n",
    "    classes: numpy.typing.NDArray[Any],\n",
    "    classes_feature_log_probs: numpy.typing.NDArray[Any],\n",
    "    n: int = 10,\n",
    "):\n",
    "    # Find the probability of the features over all classes.\n",
    "    feature_probs = numpy.sum(numpy.exp(classes_feature_log_probs), axis=0)\n",
    "\n",
    "    # For each class...\n",
    "    for class_name, class_feature_log_probs in zip(\n",
    "        classes, classes_feature_log_probs\n",
    "    ):\n",
    "        print(f\"class = {class_name}\")\n",
    "\n",
    "        # Find the probability of the features for the class.\n",
    "        class_feature_probs = numpy.vectorize(numpy.exp)(\n",
    "            class_feature_log_probs\n",
    "        )\n",
    "\n",
    "        features = sorted(\n",
    "            (\n",
    "                # Find the ratio of the probabilities of the feature for the\n",
    "                # class and over all classes.\n",
    "                (feature_name, class_feature_prob / feature_prob)\n",
    "                for feature_name, class_feature_prob, feature_prob in zip(\n",
    "                    feature_names,\n",
    "                    class_feature_probs,\n",
    "                    feature_probs,\n",
    "                )\n",
    "            ),\n",
    "            key=lambda feature: feature[1],\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        # Print the N features with the highest ratios.\n",
    "        for feature, prob_ratio in features[:n]:\n",
    "            print(f\"  {feature} = {prob_ratio:.3f}\")\n",
    "\n",
    "\n",
    "print_top_features_log_prob(\n",
    "    document_term_matrix.get_feature_names(),\n",
    "    naive_bayes.classes_,\n",
    "    naive_bayes.feature_log_prob_,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Performance metrics are just one of the ways that we need to evaluate classifiers. Metrics summarise the performance of a classifier across many different examples in the test set, but they don't tell us what the model is good at, or what kind of mistakes it makes. For this, we need to examine the errors it makes, and try to identify patterns -- this helps us to come up with improvements to the model.\n",
    "\n",
    "**TODO 2.5:** As a first error analysis step, print out some examples of misclassified tweets, along with their predicted and true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label = 1, predicted label = 0\n",
      "@user @user what do these '1/2 naked pics' have to do with anything? They're not even like that.\n",
      "true label = 0, predicted label = 1\n",
      "@user Wow,first Hugo Chavez and now Fidel Castro. Danny Glover, Michael Moore, Oliver Stone, and Sean Penn are running out of heroes.\n",
      "true label = 1, predicted label = 0\n",
      "Savchenko now Saakashvili took drug test live on Ukraine TV. To prove they are not drug-fueled loonies?\n",
      "true label = 1, predicted label = 2\n",
      "How many more days until opening day? 😩\n",
      "true label = 2, predicted label = 1\n",
      "Twitter's #ThankYouObama Shows Heartfelt Gratitude To POTUS\n",
      "true label = 0, predicted label = 1\n",
      "When Ryan privatizes SS, Medicare, Medicaid, & does away with ACA, what will Trump's base feel about \"change\" then? That's a big one right?!\n",
      "true label = 1, predicted label = 0\n",
      "@user ohhh ok i see 🤔 what if u have medical marijuana clearance? Does that make a difference\n",
      "true label = 1, predicted label = 0\n",
      "OPINION: The Anti-#Trump #Riots Are a #SmokeScreen:\n",
      "true label = 1, predicted label = 0\n",
      "ISIS and The CIA What You Need To Know! #ISIS #falseflag #saudi #CIA #mossad #israel\n",
      "true label = 1, predicted label = 0\n",
      "@user alt-right was adopted by Deplorables. Average middle Americans.  I've now moved to Libertarian. @user\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "\n",
    "\n",
    "def print_misclassified(\n",
    "    tweets: list[str],\n",
    "    labels: list[int],\n",
    "    pred_labels: numpy.typing.NDArray[Any],\n",
    "    n: int = 10,\n",
    "):\n",
    "    index = 0\n",
    "    for tweet, label, pred_label in zip(tweets, labels, pred_labels):\n",
    "        if index >= n:\n",
    "            break\n",
    "        if label != pred_label:\n",
    "            print(f\"true label = {label}, predicted label = {pred_label}\")\n",
    "            print(tweet)\n",
    "            index += 1\n",
    "\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_naive_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Logistic Regression Classifier\n",
    "\n",
    "**TODO 3.1:** Train a classifier using the [LogisticRegression class.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyright: reportUnknownMemberType=false\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(document_term_matrix.matrix, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3.2:** Obtain predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "\n",
    "pred_labels_logistic_regression = logistic_regression.predict(\n",
    "    document_term_matrix_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3.3:** Compute accuracy, precision, recall and F1 scores on the test set using [scikit-learn's metrics libary.](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label = 1, predicted label = 0\n",
      "@user @user what do these '1/2 naked pics' have to do with anything? They're not even like that.\n",
      "true label = 2, predicted label = 1\n",
      "I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
      "true label = 0, predicted label = 1\n",
      "@user Wow,first Hugo Chavez and now Fidel Castro. Danny Glover, Michael Moore, Oliver Stone, and Sean Penn are running out of heroes.\n",
      "true label = 1, predicted label = 0\n",
      "Savchenko now Saakashvili took drug test live on Ukraine TV. To prove they are not drug-fueled loonies?\n",
      "true label = 2, predicted label = 1\n",
      "Twitter's #ThankYouObama Shows Heartfelt Gratitude To POTUS\n",
      "true label = 0, predicted label = 1\n",
      "When Ryan privatizes SS, Medicare, Medicaid, & does away with ACA, what will Trump's base feel about \"change\" then? That's a big one right?!\n",
      "true label = 0, predicted label = 2\n",
      "Swampbitch Nasty Pelosi  loves yelling 'Fire' in the crowded swamp. #blackfriday @user\n",
      "true label = 1, predicted label = 0\n",
      "@user ohhh ok i see 🤔 what if u have medical marijuana clearance? Does that make a difference\n",
      "true label = 1, predicted label = 2\n",
      "@user alt-right was adopted by Deplorables. Average middle Americans.  I've now moved to Libertarian. @user\n",
      "true label = 1, predicted label = 2\n",
      "Zac Efron Flaunts Fit Abs in New ‘Dirty Grandpa’... #ZacEfron\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO 3.3:** Print out the ten features with the highest weights for each class. Hint: use the `coef_` attribute of the LogisticRegression object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class = 0\n",
      "  worst = 2.921\n",
      "  sucks = 2.721\n",
      "  horrible = 2.380\n",
      "  disappointed = 2.335\n",
      "  idiot = 2.325\n",
      "  ruined = 2.248\n",
      "  terrible = 2.242\n",
      "  bullshit = 2.238\n",
      "  disappointing = 2.217\n",
      "  stupid = 2.214\n",
      "class = 1\n",
      "  paterno = 1.604\n",
      "  dickens = 1.429\n",
      "  load = 1.327\n",
      "  bama = 1.304\n",
      "  clooney = 1.251\n",
      "  exo = 1.211\n",
      "  rush = 1.196\n",
      "  participate = 1.173\n",
      "  compare = 1.151\n",
      "  capital = 1.142\n",
      "class = 2\n",
      "  congrats = 2.656\n",
      "  exciting = 2.474\n",
      "  congratulations = 2.362\n",
      "  amazing = 2.272\n",
      "  excited = 2.249\n",
      "  brilliant = 2.118\n",
      "  impressive = 2.103\n",
      "  happy = 2.072\n",
      "  fantastic = 2.052\n",
      "  awesome = 1.907\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "\n",
    "\n",
    "def print_top_features_coef(\n",
    "    feature_names: numpy.typing.NDArray[Any],\n",
    "    classes: numpy.typing.NDArray[Any],\n",
    "    classes_coef: numpy.typing.NDArray[Any],\n",
    "    n: int = 10,\n",
    ") -> None:\n",
    "    for class_name, coefficients in zip(classes, classes_coef):\n",
    "        print(f\"class = {class_name}\")\n",
    "        features = sorted(\n",
    "            zip(feature_names, coefficients),\n",
    "            key=lambda feature: feature[1],\n",
    "            reverse=True,\n",
    "        )\n",
    "        for feature, log_prob in features[:n]:\n",
    "            print(f\"  {feature} = {log_prob:.3f}\")\n",
    "\n",
    "\n",
    "print_top_features_coef(\n",
    "    document_term_matrix.get_feature_names(),\n",
    "    logistic_regression.classes_,\n",
    "    logistic_regression.coef_,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3.4:** Print out an example of some misclassified tweets along with their predicted and true labels.\n",
    "\n",
    "**TODO 3.5:** What differences do you find between the results with NB and LR classifiers? Are there any kinds of common mistakes that either classifier makes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true label = 1, predicted label = 0\n",
      "@user @user what do these '1/2 naked pics' have to do with anything? They're not even like that.\n",
      "true label = 2, predicted label = 1\n",
      "I think I may be finally in with the in crowd #mannequinchallenge  #grads2014 @user\n",
      "true label = 0, predicted label = 1\n",
      "@user Wow,first Hugo Chavez and now Fidel Castro. Danny Glover, Michael Moore, Oliver Stone, and Sean Penn are running out of heroes.\n",
      "true label = 1, predicted label = 0\n",
      "Savchenko now Saakashvili took drug test live on Ukraine TV. To prove they are not drug-fueled loonies?\n",
      "true label = 2, predicted label = 1\n",
      "Twitter's #ThankYouObama Shows Heartfelt Gratitude To POTUS\n",
      "true label = 0, predicted label = 1\n",
      "When Ryan privatizes SS, Medicare, Medicaid, & does away with ACA, what will Trump's base feel about \"change\" then? That's a big one right?!\n",
      "true label = 0, predicted label = 2\n",
      "Swampbitch Nasty Pelosi  loves yelling 'Fire' in the crowded swamp. #blackfriday @user\n",
      "true label = 1, predicted label = 0\n",
      "@user ohhh ok i see 🤔 what if u have medical marijuana clearance? Does that make a difference\n",
      "true label = 1, predicted label = 2\n",
      "@user alt-right was adopted by Deplorables. Average middle Americans.  I've now moved to Libertarian. @user\n",
      "true label = 1, predicted label = 2\n",
      "Zac Efron Flaunts Fit Abs in New ‘Dirty Grandpa’... #ZacEfron\n"
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. N-grams and Lexicon Features\n",
    "\n",
    "We can try to improve the classifiers using some richer features.\n",
    "\n",
    "**TODO 4.1:** Use bigram features as well as unigrams (single tokens). To do these, change the `ngram_range` parameter in the CountVectorizer then try running the best classifier again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "\n",
    "from Modules.term_vectors.document_term_matrix import DocumentTermMatrix\n",
    "\n",
    "document_term_matrix_2 = DocumentTermMatrix(train_texts, ngram_range=(1, 2))\n",
    "document_term_matrix_2_test = document_term_matrix_2.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes_2 = MultinomialNB()\n",
    "naive_bayes_2.fit(document_term_matrix_2.matrix, train_labels)\n",
    "\n",
    "pred_labels_naive_bayes_2 = naive_bayes_2.predict(document_term_matrix_2_test)\n",
    "\n",
    "print_metrics(test_labels, pred_labels_naive_bayes_2)\n",
    "\n",
    "print_top_features_log_prob(\n",
    "    document_term_matrix_2.get_feature_names(),\n",
    "    naive_bayes_2.classes_,\n",
    "    naive_bayes_2.feature_log_prob_,\n",
    ")\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_naive_bayes_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression_2 = LogisticRegression()\n",
    "logistic_regression_2.fit(document_term_matrix_2.matrix, train_labels)\n",
    "\n",
    "pred_labels_logistic_regression_2 = logistic_regression_2.predict(\n",
    "    document_term_matrix_2_test\n",
    ")\n",
    "\n",
    "print_metrics(test_labels, pred_labels_logistic_regression_2)\n",
    "\n",
    "print_top_features_coef(\n",
    "    document_term_matrix_2.get_feature_names(),\n",
    "    logistic_regression_2.classes_,\n",
    "    logistic_regression_2.coef_,\n",
    ")\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_logistic_regression_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "For sentiment analysis, we can also make use of lexicons. Lexicons are lists of words associated with a particular property, such as positive sentiment. Because these lists were constructed in advance, we don't need to learn the associations between words and sentiment classes purely from the training data. This is useful because some words may be present in the test data but occur rarely, or never at all, in the training set.\n",
    "\n",
    "Here is one way we can use a lexicon to create some new features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingTypeStubs=false\n",
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "vocabulary = document_term_matrix.vocabulary\n",
    "\n",
    "lex_pos_scores = np.zeros((1, len(vocabulary)))\n",
    "lex_neg_scores = np.zeros((1, len(vocabulary)))\n",
    "\n",
    "for index, term in enumerate(vocabulary):\n",
    "    if term in analyser.lexicon and analyser.lexicon[term] > 0:\n",
    "        lex_pos_scores[0, index] = 1\n",
    "    elif term in analyser.lexicon and analyser.lexicon[term] < 0:\n",
    "        lex_neg_scores[0, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingTypeStubs=false\n",
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "lex_pos_train = np.sum(\n",
    "    document_term_matrix.matrix.multiply(lex_pos_scores), axis=1\n",
    ")\n",
    "\n",
    "print(f\"max positive train = {np.max(lex_pos_train)}\")\n",
    "\n",
    "lex_pos_test = np.sum(\n",
    "    document_term_matrix_test.multiply(lex_pos_scores), axis=1\n",
    ")\n",
    "\n",
    "print(f\"max positive test = {np.max(lex_pos_test)}\")\n",
    "\n",
    "lex_neg_train = np.sum(\n",
    "    document_term_matrix.matrix.multiply(lex_neg_scores), axis=1\n",
    ")\n",
    "\n",
    "print(f\"max negative train = {np.max(lex_neg_train)}\")\n",
    "\n",
    "lex_neg_test = np.sum(\n",
    "    document_term_matrix_test.multiply(lex_neg_scores), axis=1\n",
    ")\n",
    "\n",
    "print(f\"max negative test = {np.max(lex_neg_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can append the counts to the feature vector and treat them as extra features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportMissingTypeStubs=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "lex_train = hstack((document_term_matrix.matrix, lex_pos_train, lex_neg_train))\n",
    "lex_test = hstack((document_term_matrix_test, lex_pos_test, lex_neg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 4.2:** Use the new X_train and X_test feature vectors to train and evaluate your classifier.\n",
    "Does adding the lexicon features improve performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes_3 = MultinomialNB()\n",
    "naive_bayes_3.fit(lex_train, train_labels)\n",
    "\n",
    "pred_labels_naive_bayes_3 = naive_bayes_3.predict(lex_test)\n",
    "\n",
    "print_metrics(test_labels, pred_labels_naive_bayes_3)\n",
    "\n",
    "print_top_features_log_prob(\n",
    "    document_term_matrix.get_feature_names(),\n",
    "    naive_bayes_3.classes_,\n",
    "    naive_bayes_3.feature_log_prob_,\n",
    ")\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_naive_bayes_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportUnknownArgumentType=false\n",
    "# pyright: reportUnknownMemberType=false\n",
    "# pyright: reportUnknownVariableType=false\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression_3 = LogisticRegression()\n",
    "logistic_regression_3.fit(lex_train, train_labels)\n",
    "\n",
    "pred_labels_logistic_regression_3 = logistic_regression_3.predict(lex_test)\n",
    "\n",
    "print_metrics(test_labels, pred_labels_logistic_regression_3)\n",
    "\n",
    "print_top_features_coef(\n",
    "    document_term_matrix.get_feature_names(),\n",
    "    logistic_regression_3.classes_,\n",
    "    logistic_regression_3.coef_,\n",
    ")\n",
    "\n",
    "print_misclassified(test_texts, test_labels, pred_labels_logistic_regression_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialogue_and_narrative",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
