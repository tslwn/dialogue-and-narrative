{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sequence Tagging\n",
    "\n",
    "In this lab we will train a part-of-speech (POS) tagger using an HMM and then an RNN.\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "- Be able to train and apply an HMM.\n",
    "- Understand what the steps of Viterbi are doing.\n",
    "- Recognise how to adapt Pytorch models to use RNN layers and perform sequence tagging with neural networks.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads a POS dataset from the NLTK library.\n",
    "The second part implements and tests an HMM POS tagger.\n",
    "The third part adapts the neural network code from last week to train the RNN as a POS tagger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the PoS Tagging Data\n",
    "\n",
    "To train our POS tagger, we will use the Brown corpus, which contains many different sources of English text (books, essays, newspaper articles, government documents...) collected and hand-labelled by linguists in 1967.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the dataset into train and test, then re-format it so that each split is represented by a list of sentences and a list of tag sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use LabelEncoder to map the tokens to IDs and convert the sentences to sequences of token IDs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final preprocessing step is to map the tags (class labels) to numerical IDs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:05:25,996 pickle loading brown_tagged_sentences.pickle\n",
      "2023-11-08 14:05:26,379 pickle loaded brown_tagged_sentences.pickle\n"
     ]
    }
   ],
   "source": [
    "from Modules.sequence_tagging.brown import get_brown_tagged_sentences\n",
    "\n",
    "brown_tagged_sentences = get_brown_tagged_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Implementing the HMM\n",
    "\n",
    "Now, we are going to put together an HMM by estimating the different variables in the model from the training set.\n",
    "\n",
    "**TO-DO 2.1:** Count the state transitions and starting state occurrences in the training set and store the counts in the `transitions` and `start_states` matrices below. In `transitions`, rows correspond to states at time t-1, the columns to the following state at time t.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.2:** Normalise the transition and start state counts to estimate the conditional probabilities in the transition matrix and \\pi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.3:** Count the number of occurrences of each word type given each tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:05:26,403 hmm __initial_matrix (12,)\n",
      "2023-11-08 14:05:26,564 hmm __transition_matrix (12, 12)\n",
      "2023-11-08 14:05:26,746 hmm __emission_matrix (12, 56057)\n"
     ]
    }
   ],
   "source": [
    "from numpy import int64\n",
    "from Modules.sequence_tagging.hmm import HMMTagger\n",
    "\n",
    "hmm_tagger = HMMTagger[int64](\n",
    "    brown_tagged_sentences.n_words, brown_tagged_sentences.n_tags\n",
    ")\n",
    "\n",
    "hmm_tagger.fit(\n",
    "    brown_tagged_sentences.words_train_encoded,\n",
    "    brown_tagged_sentences.tags_train_encoded,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.4:** Normalise the observation counts to obtain the observation probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.5:** Check the implementation of viterbi below for errors!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TO-DO 2.6:** Use the viterbi function to estimate the most likely sequence of states on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will convert the predicted tag IDs to names and print the predictions along with ground truth for selected examples so we can see where it made errors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open market policy\n",
      "ADJ NOUN NOUN\n",
      "NOUN NOUN NOUN\n",
      "And you think you have language problems .\n",
      "CONJ PRON VERB PRON VERB NOUN NOUN .\n",
      "CONJ PRON VERB PRON VERB NOUN NOUN .\n",
      "Mae entered the room from the hallway to the kitchen .\n",
      "NOUN VERB DET NOUN ADP DET NOUN ADP DET NOUN .\n",
      "NOUN VERB DET NOUN ADP DET NOUN PRT DET NOUN .\n",
      "This will permit you to get a rough estimate of how much the materials for the shell will cost .\n",
      "DET VERB VERB PRON PRT VERB DET ADJ NOUN ADP ADV ADJ DET NOUN ADP DET NOUN VERB VERB .\n",
      "DET VERB VERB PRON PRT VERB DET ADJ NOUN ADP ADV ADV DET NOUN ADP DET NOUN VERB NOUN .\n",
      "the multifigure `` Traveling Carnival '' , in which action is vivified by lighting ; ;\n",
      "DET NOUN . VERB NOUN . . ADP DET NOUN VERB VERB ADP VERB . .\n",
      "DET NOUN . VERB NOUN . . ADP DET NOUN VERB NOUN ADP NOUN . .\n"
     ]
    }
   ],
   "source": [
    "# Print the predicted tags for the first N sentences.\n",
    "N_SENTENCES = 5\n",
    "for test_words, test_tags in zip(\n",
    "    brown_tagged_sentences.words_test_encoded[:N_SENTENCES],\n",
    "    brown_tagged_sentences.tags_test_encoded[:N_SENTENCES],\n",
    "):\n",
    "    pred_tags = list(\n",
    "        hmm_tagger.predict(\n",
    "            test_words,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    test_words_decoded = brown_tagged_sentences.decode_words(test_words)\n",
    "    test_tags_decoded = brown_tagged_sentences.decode_tags(test_tags)\n",
    "    pred_tags_decoded = brown_tagged_sentences.decode_tags(pred_tags)\n",
    "\n",
    "    print(\" \".join(test_words_decoded))\n",
    "    print(\" \".join(test_tags_decoded))\n",
    "    print(\" \".join(pred_tags_decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well it did overall by computing performance metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.417\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "# Compute the accuracy for the test set.\n",
    "correct: int = 0\n",
    "for test_words, test_tags in zip(\n",
    "    brown_tagged_sentences.words_test_encoded,\n",
    "    brown_tagged_sentences.tags_test_encoded,\n",
    "):\n",
    "    pred_tags = list(\n",
    "        hmm_tagger.predict(\n",
    "            test_words,\n",
    "        )\n",
    "    )\n",
    "    correct += numpy.sum(pred_tags == test_tags)\n",
    "\n",
    "accuracy = correct / len(brown_tagged_sentences.tags_test_encoded)\n",
    "print(f\"accuracy {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. POS Tagging with an RNN\n",
    "\n",
    "The code below is adapted from last week's text classifier code to first pad the sequences, then format them into DataLoader objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:05:33,452 pickle loading brown_tagged_sentences_padded.pickle\n",
      "2023-11-08 14:05:33,464 pickle loaded brown_tagged_sentences_padded.pickle\n"
     ]
    }
   ],
   "source": [
    "from Modules.sequence_tagging.rnn import get_brown_tagged_sentences_padded\n",
    "\n",
    "(\n",
    "    words_train_padded,\n",
    "    words_test_padded,\n",
    "    tags_train_padded,\n",
    "    tags_test_padded,\n",
    "    n_words,\n",
    "    n_tags,\n",
    ") = get_brown_tagged_sentences_padded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from Modules.sequence_tagging.util import to_data_loader\n",
    "\n",
    "data_loader_train = to_data_loader(words_train_padded, tags_train_padded)\n",
    "data_loader_test = to_data_loader(words_test_padded, tags_test_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we're going to create a neural network sequence tagger using an RNN layer. This will be based on the code we used last time, with two key differences:\n",
    "\n",
    "- Including an RNN hidden layer\n",
    "- The output will have an additional dimension of size sequence_length, so that it can provide predictions for every token in the sequence.\n",
    "\n",
    "**TODO 3.1:** Complete the code below to change the hidden layer to a single RNN layer. See [the documentation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we can run the code below to train and test the RNN model. This uses basically the same code as last week.\n",
    "\n",
    "**TO-DO 3.2:** What is wrong with comparing the RNN tagger's performance computed here with that of the HMM? Hint: all the sequences are length 40.\n",
    "\n",
    "**TO-DO 3.3:** Can you fix the accuracy computations to make them comparable with the accuracy for the HMM?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs the trainin process by calling train_nn():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements a testing or prediction function and computes accuracy.\n",
    "\n",
    "**TO-DO 3.4:** Adjust the code below to correctly compute the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 717/717 [00:04<00:00, 178.25it/s]\n",
      "2023-11-08 14:05:38,630 rnn epoch 1\n",
      "2023-11-08 14:05:38,631 rnn training loss 1.399\n",
      "2023-11-08 14:05:38,631 rnn training accuracy 27.1%\n",
      "2023-11-08 14:05:38,895 rnn validation loss 0.870\n",
      "2023-11-08 14:05:38,895 rnn validation accuracy 34.7%\n",
      "100%|██████████| 717/717 [00:03<00:00, 185.82it/s]\n",
      "2023-11-08 14:05:42,755 rnn epoch 2\n",
      "2023-11-08 14:05:42,755 rnn training loss 0.685\n",
      "2023-11-08 14:05:42,756 rnn training accuracy 37.7%\n",
      "2023-11-08 14:05:43,016 rnn validation loss 0.567\n",
      "2023-11-08 14:05:43,016 rnn validation accuracy 39.4%\n",
      "100%|██████████| 717/717 [00:03<00:00, 184.86it/s]\n",
      "2023-11-08 14:05:46,896 rnn epoch 3\n",
      "2023-11-08 14:05:46,896 rnn training loss 0.490\n",
      "2023-11-08 14:05:46,897 rnn training accuracy 40.7%\n",
      "2023-11-08 14:05:47,159 rnn validation loss 0.444\n",
      "2023-11-08 14:05:47,159 rnn validation accuracy 41.4%\n",
      "100%|██████████| 717/717 [00:03<00:00, 182.19it/s]\n",
      "2023-11-08 14:05:51,096 rnn epoch 4\n",
      "2023-11-08 14:05:51,096 rnn training loss 0.392\n",
      "2023-11-08 14:05:51,096 rnn training accuracy 42.2%\n",
      "2023-11-08 14:05:51,355 rnn validation loss 0.370\n",
      "2023-11-08 14:05:51,355 rnn validation accuracy 42.5%\n",
      "100%|██████████| 717/717 [00:03<00:00, 184.28it/s]\n",
      "2023-11-08 14:05:55,247 rnn epoch 5\n",
      "2023-11-08 14:05:55,247 rnn training loss 0.329\n",
      "2023-11-08 14:05:55,247 rnn training accuracy 43.2%\n",
      "2023-11-08 14:05:55,498 rnn validation loss 0.322\n",
      "2023-11-08 14:05:55,498 rnn validation accuracy 43.2%\n",
      "100%|██████████| 717/717 [00:03<00:00, 182.73it/s]\n",
      "2023-11-08 14:05:59,423 rnn epoch 6\n",
      "2023-11-08 14:05:59,424 rnn training loss 0.285\n",
      "2023-11-08 14:05:59,424 rnn training accuracy 43.9%\n",
      "2023-11-08 14:05:59,671 rnn validation loss 0.287\n",
      "2023-11-08 14:05:59,671 rnn validation accuracy 43.8%\n",
      "100%|██████████| 717/717 [00:03<00:00, 183.68it/s]\n",
      "2023-11-08 14:06:03,575 rnn epoch 7\n",
      "2023-11-08 14:06:03,576 rnn training loss 0.251\n",
      "2023-11-08 14:06:03,576 rnn training accuracy 44.5%\n",
      "2023-11-08 14:06:03,840 rnn validation loss 0.261\n",
      "2023-11-08 14:06:03,840 rnn validation accuracy 44.2%\n",
      "100%|██████████| 717/717 [00:03<00:00, 184.78it/s]\n",
      "2023-11-08 14:06:07,721 rnn epoch 8\n",
      "2023-11-08 14:06:07,721 rnn training loss 0.225\n",
      "2023-11-08 14:06:07,722 rnn training accuracy 44.9%\n",
      "2023-11-08 14:06:07,968 rnn validation loss 0.242\n",
      "2023-11-08 14:06:07,968 rnn validation accuracy 44.6%\n",
      "100%|██████████| 717/717 [00:03<00:00, 183.01it/s]\n",
      "2023-11-08 14:06:11,887 rnn epoch 9\n",
      "2023-11-08 14:06:11,887 rnn training loss 0.203\n",
      "2023-11-08 14:06:11,887 rnn training accuracy 45.2%\n",
      "2023-11-08 14:06:12,144 rnn validation loss 0.227\n",
      "2023-11-08 14:06:12,144 rnn validation accuracy 44.8%\n",
      "100%|██████████| 717/717 [00:03<00:00, 185.15it/s]\n",
      "2023-11-08 14:06:16,017 rnn epoch 10\n",
      "2023-11-08 14:06:16,018 rnn training loss 0.186\n",
      "2023-11-08 14:06:16,018 rnn training accuracy 45.5%\n",
      "2023-11-08 14:06:16,266 rnn validation loss 0.214\n",
      "2023-11-08 14:06:16,266 rnn validation accuracy 45.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Modules.sequence_tagging.rnn import RNNTagger\n",
    "\n",
    "EMBEDDING_DIM = 25\n",
    "HIDDEN_DIM = 32\n",
    "HIDDEN_LAYERS = 1\n",
    "LEARNING_RATE = 0.0005\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Ignore the padding index when computing the loss.\n",
    "cross_entropy_loss = torch.nn.CrossEntropyLoss(ignore_index=n_words)\n",
    "\n",
    "rnn_tagger = RNNTagger(\n",
    "    loss_fn=cross_entropy_loss,\n",
    "    # Include the padding index in the input.\n",
    "    n_words=n_words + 1,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    hidden_layers=HIDDEN_LAYERS,\n",
    "    # Include the padding index in the output.\n",
    "    output_dim=n_tags + 1,\n",
    ")\n",
    "\n",
    "adam_optimizer = torch.optim.Adam(rnn_tagger.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "rnn_tagger.train_(\n",
    "    n_epochs=N_EPOCHS,\n",
    "    train_loader=data_loader_train,\n",
    "    val_loader=data_loader_test,\n",
    "    optimizer=adam_optimizer,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
