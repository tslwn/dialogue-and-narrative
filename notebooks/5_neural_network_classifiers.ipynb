{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Neural Networks for Text Classification\n",
    "\n",
    "This lab introduces (deep) neural networks for text classification using Pytorch, and applies it to the datasets we previously used with na√Øve Bayes and logistic regression. Pytorch is a framework for machine learning with neural networks, which is widely used in fields such as Computer Vision and NLP.\n",
    "\n",
    "You may also find [Pytorch's tutorials](https://pytorch.org/tutorials/) useful to give more depth on different parts of the framework.\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "- Be able to construct and train a neural network classifier in Pytorch.\n",
    "- Understand how to use word embeddings as input to a neural network.\n",
    "- Know how to compare classifier performance on a test set.\n",
    "\n",
    "### Overview\n",
    "\n",
    "We first format the data so it can be input to the neural network. Then we see how to construct a neural network with Pytorch, then train and test it. Finally, we introduce pretrained embeddings to the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the Data\n",
    "\n",
    "This section contains the same loader code as earlier labs, which loads the sentiment dataset from TweetEval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tweet_eval (/Users/qr23940/git/dialogue_and_narrative/notebooks/data_cache/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "/Users/qr23940/miniconda3/envs/dn/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "Found cached dataset tweet_eval (/Users/qr23940/git/dialogue_and_narrative/notebooks/data_cache/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "Found cached dataset tweet_eval (/Users/qr23940/git/dialogue_and_narrative/notebooks/data_cache/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    }
   ],
   "source": [
    "from Modules.neural_network_classifiers.feedforward_tweet_eval import (\n",
    "    prepare_data,\n",
    ")\n",
    "\n",
    "sequence_length = 40\n",
    "batch_size = 64\n",
    "\n",
    "(\n",
    "    tokenizer,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    dev_loader,\n",
    "    num_embeddings,\n",
    "    output_dim,\n",
    ") = prepare_data(sequence_length=sequence_length, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparing the Data\n",
    "\n",
    "Now we put the dataset into a suitable format for a Pytorch NN classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As inputs to the Sklearn classifiers in week 3, we used CountVectorizer to extract a single vector representation for a _whole document_.\n",
    "However, one motivation for using a neural network is that it can process the individual words in the sentence in order, and learn how to combine information from different tokens automatically. This means we don't need to convert the document to a fixed-length vector during the preprocessing phase.\n",
    "Instead, as input to our neural network, we will pass in a sequence of tokens, where each token is represented by its _input_id_, which is its index into the vocabulary.\n",
    "\n",
    "The first step is to compute the vocabulary. This can be done in various ways, but here we will stick with the familiar CountVectorizer method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to map the tokens to their IDs -- their indexes in the vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network's input layer has a fixed size, so we need to somehow make all of our documents have the same number of tokens. We can do this by setting a fixed sequence length, then _padding_ the short documents with a special token. Any documents that exceed the length will be truncated. Let's plot a histogram to understand the length distribution of the texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The code cell below in intended to pad any documents that are too short and truncate any that are too long, so that we obtain a set of sequences of equal length.\n",
    "\n",
    "**TODO 2.1:** Complete the padding code below to insert 0s at the start of any sequences that are too short, and to truncate any sequences that are too long.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our data in the right format. When training, the neural network will process the data in randomly-chosen mini-batches, rather than all at once.\n",
    "To enable this, we wrap our dataset in a DataLoader, which allows the network to select batches of data:\n",
    "\n",
    "DataLoader: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Constructing the Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a NN with three different layers for sentiment classification.\n",
    "\n",
    "### Embedding layer\n",
    "\n",
    "In the embedding layer, the network will create its own embeddings for the index with a given embedding dimension.\n",
    "The module `nn.Embedding()` creates a simple lookup table that stores embeddings of words in a fixed dictionary with fixed size.\n",
    "This module is often used to store word embeddings and retrieve them using indices.\n",
    "The module's input is a list of indices, and the output is the corresponding word embeddings.\n",
    "\n",
    "[Documentation for Embedding Class](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "\n",
    "### Fully-connected layer\n",
    "\n",
    "Fully-connected layers in a neural network are those layers where all the inputs from the previous layer are connected to every unit of the fully-connected layer.\n",
    "Here we will use fully-connected layers for the hidden layer and output layer. In Pytorch this kind of layer is implemented by the 'Linear' class. The name 'linear' is used because the nonlinear part is provided by the activation functions, which act like another layer in the network.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "## Activation functions\n",
    "\n",
    "In Pytorch, the activation function is not included in the Linear class (or other kinds of neural network layer). An example of an activation function is ReLU, which is commonly used in the hidden layers of a neural network:\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "\n",
    "In Pytorch, we construct a neural network by connecting up the output of each component to the input of the next, thereby creating a computation graph.\n",
    "To complete a fully-connected hidden layer, we connect the ouput of a Linear layer to the input of a ReLU activation function, thereby creating a nonlinear function.\n",
    "\n",
    "**TODO 3.1** Complete the constructor for a NN with three layers by adding the missing dimensions.\n",
    "\n",
    "**TODO 3.2** Complete the forward function that maps the input data to an output by adding the missing line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3.3** Create a NN with the FFTextClassifier class we wrote.\n",
    "\n",
    "**Hint:** `model = FFTextClassifier(...)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After desigining our network, we need to create a training function to calculate the loss for each input and perform backpropagation to optimise the network.\n",
    "During training, the weights of all the layers will be updated.\n",
    "\n",
    "We build a training function to train the NN over a fixed number of epochs (an epoch is one iteration over the whole training dataset).\n",
    "The function also prints the performance of both training and development/validation set after each epoch. There are some high-level wrapper libraries that do this stuff for you, but when learning about neural networks, it's useful to see what's going on inside.\n",
    "\n",
    "**TODO 3.4** Complete the code below to compute the validation accuracy and loss after each training epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before we start training is defining the loss function and optimizer.\n",
    "\n",
    "Here we use cross-entropy loss and the Adam optimizer (it tends to find a better solution in a small number of iterations than SGD).\n",
    "The module `nn.CrossEntropyLoss()` combines `LogSoftmax` and `NLLLoss` in one single class so that we don't have to implement the softmax layer within the forward() method.\n",
    "\n",
    "Cross Entropy Loss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "Optimization: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "**TODO 3.4** Finally, train the network for 10 epochs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0\n",
      "Training loss = 1.281\n",
      "Training accuracy = 42.2 %\n",
      "Validation loss = 1.272\n",
      "Validation accuracy = 43.3 %\n",
      "Epoch = 1\n",
      "Training loss = 1.201\n",
      "Training accuracy = 47.1 %\n",
      "Validation loss = 1.251\n",
      "Validation accuracy = 45.2 %\n",
      "Epoch = 2\n",
      "Training loss = 1.153\n",
      "Training accuracy = 48.8 %\n",
      "Validation loss = 1.284\n",
      "Validation accuracy = 39.3 %\n",
      "Epoch = 3\n",
      "Training loss = 1.105\n",
      "Training accuracy = 52.3 %\n",
      "Validation loss = 1.329\n",
      "Validation accuracy = 35.8 %\n",
      "Epoch = 4\n",
      "Training loss = 1.069\n",
      "Training accuracy = 53.5 %\n",
      "Validation loss = 1.310\n",
      "Validation accuracy = 39.3 %\n",
      "Epoch = 5\n",
      "Training loss = 1.028\n",
      "Training accuracy = 55.8 %\n",
      "Validation loss = 1.318\n",
      "Validation accuracy = 39.3 %\n",
      "Epoch = 6\n",
      "Training loss = 0.982\n",
      "Training accuracy = 59.2 %\n",
      "Validation loss = 1.369\n",
      "Validation accuracy = 37.4 %\n",
      "Epoch = 7\n",
      "Training loss = 0.951\n",
      "Training accuracy = 60.3 %\n",
      "Validation loss = 1.397\n",
      "Validation accuracy = 40.6 %\n",
      "Epoch = 8\n",
      "Training loss = 0.915\n",
      "Training accuracy = 62.2 %\n",
      "Validation loss = 1.397\n",
      "Validation accuracy = 41.2 %\n",
      "Epoch = 9\n",
      "Training loss = 0.874\n",
      "Training accuracy = 63.9 %\n",
      "Validation loss = 1.428\n",
      "Validation accuracy = 39.6 %\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "from Modules.neural_network_classifiers.feedforward import (\n",
    "    FeedforwardTextClassifier,\n",
    ")\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "embedding_dim = 25\n",
    "hidden_dim = 32\n",
    "\n",
    "model = FeedforwardTextClassifier(\n",
    "    loss_fn=loss_fn,\n",
    "    num_embeddings=num_embeddings,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0005\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train_(\n",
    "    num_epochs=num_epochs,\n",
    "    train_loader=train_loader,\n",
    "    dev_loader=dev_loader,\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3.5:** Evaluate the model on test set using the function below. Complete the code to count the correct classifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss = 1.382\n",
      "Test accuracy = 40.8 %\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.test_(loader=test_loader)\n",
    "\n",
    "print(f\"Test loss = {test_loss:.3f}\")\n",
    "print(f\"Test accuracy = {test_accuracy:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pretrained Embeddings\n",
    "\n",
    "Now let's use pretrained word embeddings as inputs instead of learning them from scratch during training.\n",
    "Here, we will use a pretrained embedding matrix to initialise the embedding layer, which will then be updated during training.\n",
    "\n",
    "The class below extends the FFTextClassifier class. This means that it inherits all of its functionality, but we now overwrite the constructor (the `__init__` method).\n",
    "This way, we don't need to define the forward function again, as it will be the same as before.\n",
    "\n",
    "**TODO 4.1** As before, complete the arguments below to set the dimensions of the neural network layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO 4.2** Using the above class, construct, train and test the classifier with pretrained embeddings. You will need to create a new optimizer object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qr23940/git/dialogue_and_narrative/Modules/neural_network_classifiers/feedforward_embeddings.py:47: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1695391829313/work/torch/csrc/utils/tensor_numpy.cpp:212.)\n",
      "  ] = torch.from_numpy(  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0\n",
      "Training loss = 1.285\n",
      "Training accuracy = 41.7 %\n",
      "Validation loss = 1.244\n",
      "Validation accuracy = 42.8 %\n",
      "Epoch = 1\n",
      "Training loss = 1.210\n",
      "Training accuracy = 45.4 %\n",
      "Validation loss = 1.208\n",
      "Validation accuracy = 42.8 %\n",
      "Epoch = 2\n",
      "Training loss = 1.163\n",
      "Training accuracy = 49.6 %\n",
      "Validation loss = 1.215\n",
      "Validation accuracy = 43.3 %\n",
      "Epoch = 3\n",
      "Training loss = 1.133\n",
      "Training accuracy = 50.1 %\n",
      "Validation loss = 1.215\n",
      "Validation accuracy = 45.7 %\n",
      "Epoch = 4\n",
      "Training loss = 1.084\n",
      "Training accuracy = 53.5 %\n",
      "Validation loss = 1.214\n",
      "Validation accuracy = 42.0 %\n",
      "Epoch = 5\n",
      "Training loss = 1.029\n",
      "Training accuracy = 57.4 %\n",
      "Validation loss = 1.197\n",
      "Validation accuracy = 44.1 %\n",
      "Epoch = 6\n",
      "Training loss = 0.985\n",
      "Training accuracy = 60.1 %\n",
      "Validation loss = 1.205\n",
      "Validation accuracy = 45.5 %\n",
      "Epoch = 7\n",
      "Training loss = 0.935\n",
      "Training accuracy = 62.1 %\n",
      "Validation loss = 1.228\n",
      "Validation accuracy = 45.7 %\n",
      "Epoch = 8\n",
      "Training loss = 0.887\n",
      "Training accuracy = 64.7 %\n",
      "Validation loss = 1.223\n",
      "Validation accuracy = 46.3 %\n",
      "Epoch = 9\n",
      "Training loss = 0.841\n",
      "Training accuracy = 66.9 %\n",
      "Validation loss = 1.267\n",
      "Validation accuracy = 46.3 %\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from Modules.neural_network_classifiers.feedforward_embeddings import (\n",
    "    FeedforwardTextClassifierEmbeddings,\n",
    ")\n",
    "\n",
    "model = FeedforwardTextClassifierEmbeddings(\n",
    "    loss_fn=loss_fn,\n",
    "    num_embeddings=num_embeddings,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    sequence_length=sequence_length,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train_(\n",
    "    num_epochs=num_epochs,\n",
    "    train_loader=train_loader,\n",
    "    dev_loader=dev_loader,\n",
    "    optimizer=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss = 1.325\n",
      "Test accuracy = 47.6 %\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.test_(loader=test_loader)\n",
    "\n",
    "print(f\"Test loss = {test_loss:.3f}\")\n",
    "print(f\"Test accuracy = {test_accuracy:.1f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialogue_and_narrative",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
