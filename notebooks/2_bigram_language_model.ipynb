{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n",
    "\n",
    "In this lab we will implement a bigram language model and use it to compute the probability of some sample sentences.\n",
    "\n",
    "As you go through, make sure you understand what's going on in each cell, and ask if it is unclear.\n",
    "\n",
    "### Outcomes\n",
    "\n",
    "- Know how to count word frequencies in a corpus using Python libraries.\n",
    "- Understand how to compute conditional probabilities.\n",
    "- Be able to apply the chain rule to compute the probability of a sentence.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads the same dataset as last week.\n",
    "The next part splits the data into training and test sets, and tokenises the utterances.\n",
    "After this there are some tasks to complete to implement and test the language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def generate_examples(filepath: str):\n",
    "    \"\"\"\n",
    "    Adapted from https://huggingface.co/datasets/doc2dial/blob/main/doc2dial.py#L226\n",
    "    \"\"\"\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        for domain in data[\"dial_data\"]:\n",
    "            for doc_id in data[\"dial_data\"][domain]:\n",
    "                for dialogue in data[\"dial_data\"][domain][doc_id]:\n",
    "                    x = {\n",
    "                        \"dial_id\": dialogue[\"dial_id\"],\n",
    "                        \"domain\": domain,\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"turns\": dialogue[\"turns\"],\n",
    "                    }\n",
    "\n",
    "                    yield dialogue[\"dial_id\"], x\n",
    "\n",
    "\n",
    "dataset = [\n",
    "    *generate_examples(\n",
    "        # The filepath of the training data from the repository:\n",
    "        # https://github.com/doc2dial/sharedtask-dialdoc2021\n",
    "        \"../../../sharedtask-dialdoc2021/data/doc2dial/v1.0.1/doc2dial_dial_train.json\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances: 1195\n"
     ]
    }
   ],
   "source": [
    "# Collect all of the utterances into a list. For this task, we don't care about\n",
    "# the order of the utterances in the conversation - we will just use them as\n",
    "# examples of the language we want to model.\n",
    "docs = [\n",
    "    turn[\"utterance\"] for item in dataset[:100] for turn in item[1][\"turns\"]\n",
    "]\n",
    "\n",
    "print(f\"Number of utterances: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/qr23940/miniconda3/envs/dialogue_and_narrative/lib/python3.11/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our result:\n",
      "['<s>', 'Can', 'I', 'do', 'my', 'DMV', 'transactions', 'online', '?', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import spacy\n",
    "import typing\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenize the utterances.\n",
    "docs_tokenized_unpadded = [[token.text for token in nlp(doc)] for doc in docs]\n",
    "\n",
    "token_start = \"<s>\"\n",
    "token_end = \"</s>\"\n",
    "\n",
    "\n",
    "# Add artificial start `<s>` and end `</s>` tokens to each utterance. These will\n",
    "# be used to compute the conditional probabilities of each word at the start of\n",
    "# a sentence and the conditional probabilities of ending the sentence after a\n",
    "# particular word. This lets us model which words are most likely to start or\n",
    "# end a sentence.\n",
    "def pad(docs: typing.List[typing.List[str]]) -> typing.List[typing.List[str]]:\n",
    "    return [[token_start] + doc + [token_end] for doc in docs]\n",
    "\n",
    "\n",
    "docs_tokenized = pad(docs_tokenized_unpadded)\n",
    "\n",
    "# Print an example of a tokenized utterance.\n",
    "doc_tokenized_example_1 = docs_tokenized[2]\n",
    "print(\"Our result:\")\n",
    "pprint(doc_tokenized_example_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK's result:\n",
      "['<s>', 'Can', 'I', 'do', 'my', 'DMV', 'transactions', 'online', '?', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Compare the result to NLTK's.\n",
    "docs_tokenized_nltk = [\n",
    "    pad_both_ends(\n",
    "        doc_tokenized_unpadded,\n",
    "        n=2,\n",
    "    )\n",
    "    for doc_tokenized_unpadded in docs_tokenized_unpadded\n",
    "]\n",
    "print(\"NLTK's result:\")\n",
    "pprint(list(docs_tokenized_nltk[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The training set has 1195 samples and the test set has 0 samples.'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets with `scikit-learn`. Note that we\n",
    "# split the unpadded documents *and then* pad them so that we use the same split\n",
    "# to evaluate our results and NLTK's.\n",
    "# data_train_unpadded, data_test_unpadded = train_test_split(\n",
    "#     docs_tokenized_unpadded, train_size=0.8, test_size=0.2\n",
    "# )\n",
    "data_train_unpadded = docs_tokenized_unpadded\n",
    "data_test_unpadded = []\n",
    "data_train = pad(data_train_unpadded)\n",
    "data_test = pad(data_test_unpadded)\n",
    "\n",
    "pprint(\n",
    "    f\"The training set has {len(data_train)} samples and the test set has {len(data_test)} samples.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Counting Tokens\n",
    "\n",
    "The bigram language model needs to compute two sets of counts from the training data:\n",
    "\n",
    "1. The counts of how many times each bigram occurs.\n",
    "2. The counts of how many times each word type occurs as the first token in a bigram.\n",
    "\n",
    "Let's start by finding the vocabulary of unique token 'types':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['\\n', '\\n   ', ' ', ..., 'yourself', '¡', '´'], dtype='<U40')\n",
      "'There are 1838 types in our vocabulary.'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "vocab = np.unique(np.concatenate(data_train))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "pprint(vocab)\n",
    "pprint(f\"There are {vocab_size} types in our vocabulary.\")\n",
    "\n",
    "# So far, `vocab` is a numpy array. It may be simpler to work with a list:\n",
    "vocab = list(vocab.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import typing\n",
    "\n",
    "\n",
    "# For example, we can find the index of a token like so:\n",
    "def find_token_index(token: str, vocab: list[str]):\n",
    "    try:\n",
    "        return vocab.index(token)\n",
    "        # If `vocab` were a numpy array, we could find the token's index like so:\n",
    "        # `return np.argwhere(vocab == token)[0][0]`\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "pprint(find_token_index(token_start, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.typing as npt\n",
    "\n",
    "\n",
    "# Define a function to print the bigram statistics of a tokenized utterance.\n",
    "def print_bigram_statistics(\n",
    "    bigram_matrix: npt.NDArray, tokenized: typing.List[str]\n",
    "):\n",
    "    bigram_statistics = []\n",
    "    for token_current, token_next in zip(\n",
    "        tokenized,\n",
    "        tokenized[1:],\n",
    "    ):\n",
    "        # Find the indices of the tokens in the vocabulary.\n",
    "        token_current_index = find_token_index(token_current, vocab)\n",
    "        token_next_index = find_token_index(token_next, vocab)\n",
    "\n",
    "        # If both tokens are in the vocabulary, find the value of the statistic\n",
    "        # for the bigram and add it to the list to print.\n",
    "        if token_current_index != -1 and token_next_index != -1:\n",
    "            bigram_statistic = bigram_matrix[\n",
    "                token_current_index, token_next_index\n",
    "            ]\n",
    "            bigram_statistics.append(\n",
    "                f\"{token_current} {token_next}: {bigram_statistic}\"\n",
    "            )\n",
    "\n",
    "    pprint(bigram_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.1:** count the bigrams that occur in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our result:\n",
      "['<s> Can: 48.0',\n",
      " 'Can I: 28.0',\n",
      " 'I do: 52.0',\n",
      " 'do my: 4.0',\n",
      " 'my DMV: 4.0',\n",
      " 'DMV transactions: 3.0',\n",
      " 'transactions online: 2.0',\n",
      " 'online ?: 9.0',\n",
      " '? </s>: 484.0']\n"
     ]
    }
   ],
   "source": [
    "def find_bigram_counts(\n",
    "    utterances: typing.List[typing.List[str]], vocab_size: int\n",
    "):\n",
    "    # A matrix whose row indices correspond to the first tokens in bigrams and\n",
    "    # column indices correspond to the second tokens in bigrams. The indices must\n",
    "    # map to the index of the token in the vocabulary. The values of the matrix will\n",
    "    # be the token counts. We initialize the matrix with ones to use add-one\n",
    "    # smoothing.\n",
    "    bigram_counts = np.ones((vocab_size, vocab_size))\n",
    "\n",
    "    tokens = [token for utterance in utterances for token in utterance]\n",
    "\n",
    "    # Iterate the tokens in each utterance pairwise.\n",
    "    for token_current, token_next in zip(tokens, tokens[1:]):\n",
    "        # Find the indices of the tokens in the vocabulary.\n",
    "        token_current_index = find_token_index(token_current, vocab)\n",
    "        token_next_index = find_token_index(token_next, vocab)\n",
    "\n",
    "        # If both tokens are in the vocabulary, increment the bigram count.\n",
    "        if token_current_index != -1 and token_next_index != -1:\n",
    "            bigram_counts[token_current_index, token_next_index] += 1\n",
    "\n",
    "    return bigram_counts\n",
    "\n",
    "\n",
    "bigram_counts = find_bigram_counts(data_train, vocab_size)\n",
    "\n",
    "# Print the counts of the bigrams in an example of a tokenized utterance.\n",
    "print(\"Our result:\")\n",
    "print_bigram_statistics(bigram_counts, doc_tokenized_example_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK's result:\n",
      "['<s> Can: 48',\n",
      " 'Can I: 28',\n",
      " 'I do: 52',\n",
      " 'do my: 4',\n",
      " 'my DMV: 4',\n",
      " 'DMV transactions: 3',\n",
      " 'transactions online: 2',\n",
      " 'online ?: 9',\n",
      " '? </s>: 484']\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.models import Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.util import bigrams\n",
    "from pprint import pprint\n",
    "\n",
    "# Compare the result to NLTK's.\n",
    "laplace = Laplace(order=2)\n",
    "laplace.fit(*padded_everygram_pipeline(order=2, text=data_train_unpadded))\n",
    "\n",
    "print(\"NLTK's result:\")\n",
    "pprint(\n",
    "    [\n",
    "        # NLTK doesn't include the pseudo-counts introduced by Laplace (add-one)\n",
    "        # or Lidstone (add-k) smoothing in its `counts` attribute, so add one.\n",
    "        # It does include the pseudo-counts in its `unmasked_score` method (and,\n",
    "        # therefore, the dependent `score`, `logscore`, etc. methods.\n",
    "        # See here: https://www.nltk.org/_modules/nltk/lm/models.html#Lidstone.unmasked_score\n",
    "        f\"{token_current} {token_next}: {laplace.counts[[token_current]][token_next] + 1}\"\n",
    "        for token_current, token_next in list(bigrams(doc_tokenized_example_1))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.2:** Apply numpy's sum() function to the 'counts' variable to compute the number of times each word type occurs as the first token in a bigram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigram first-token counts:\n",
      "['<s>: 3033.0',\n",
      " 'Can: 1894.0',\n",
      " 'I: 2367.0',\n",
      " 'do: 2034.0',\n",
      " 'my: 2112.0',\n",
      " 'DMV: 2048.0',\n",
      " 'transactions: 1848.0',\n",
      " 'online: 1884.0',\n",
      " '?: 2353.0',\n",
      " '</s>: 3032.0']\n",
      "\n",
      "Unigram counts:\n",
      "['<s>: 1195.0',\n",
      " 'Can: 56.0',\n",
      " 'I: 529.0',\n",
      " 'do: 196.0',\n",
      " 'my: 274.0',\n",
      " 'DMV: 210.0',\n",
      " 'transactions: 10.0',\n",
      " 'online: 46.0',\n",
      " '?: 515.0',\n",
      " '</s>: 1195.0']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# `axis=1`: apply the operation row-wise, i.e., across all rows for each column.\n",
    "first_token_counts = bigram_counts.sum(axis=1)\n",
    "\n",
    "\n",
    "# Compute the unigram counts. The result should be the same.\n",
    "def find_unigram_counts(\n",
    "    utterances: typing.List[typing.List[str]], vocab_size: int\n",
    "):\n",
    "    # A vector whose indices correspond to the tokens in the vocabulary.\n",
    "    unigram_counts = np.zeros(vocab_size)\n",
    "\n",
    "    # Iterate the tokens in the utterances.\n",
    "    for token in [token for utterance in utterances for token in utterance]:\n",
    "        # Find the index of the token in the vocabulary.\n",
    "        token_index = find_token_index(token, vocab)\n",
    "\n",
    "        # If the token is in the vocabulary, increment the unigram count.\n",
    "        if token_index != -1:\n",
    "            unigram_counts[token_index] += 1\n",
    "\n",
    "    return unigram_counts\n",
    "\n",
    "\n",
    "unigram_counts = find_unigram_counts(data_train, vocab_size)\n",
    "\n",
    "# Print the counts with which the tokens in an example of a tokenized utterance\n",
    "# occur as the first token in a bigram.\n",
    "print(\"\\nBigram first-token counts:\")\n",
    "pprint(\n",
    "    [\n",
    "        f\"{token}: {first_token_counts[find_token_index(token, vocab)]}\"\n",
    "        for token in doc_tokenized_example_1\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the unigram counts, which are smaller than the bigram first-token counts\n",
    "# by the size of the vocabulary.\n",
    "print(\"\\nUnigram counts:\")\n",
    "pprint(\n",
    "    [\n",
    "        f\"{token}: {unigram_counts[find_token_index(token, vocab)]}\"\n",
    "        for token in doc_tokenized_example_1\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK's result:\n",
      "\n",
      "Unigram counts:\n",
      "['<s>: 1195',\n",
      " 'Can: 56',\n",
      " 'I: 529',\n",
      " 'do: 196',\n",
      " 'my: 274',\n",
      " 'DMV: 210',\n",
      " 'transactions: 10',\n",
      " 'online: 46',\n",
      " '?: 515',\n",
      " '</s>: 1195']\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK's result:\")\n",
    "print(\"\\nUnigram counts:\")\n",
    "pprint(\n",
    "    [\n",
    "        f\"{token}: {laplace.counts.unigrams[token]}\"\n",
    "        for token in doc_tokenized_example_1\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Check our results are the same.\n",
    "for index, count in enumerate(unigram_counts):\n",
    "    if count != laplace.counts.unigrams[vocab[index]]:\n",
    "        print(\n",
    "            f\"{vocab[index]}: {count} != {laplace.counts.unigrams[vocab[index]]}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.3:** Compute a matrix (numpy array) of conditional probabilities using the counts. Compute the log of this matrix as a variable 'log_cond_probs'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our results:\n",
      "\n",
      "Bigram conditional probabilities:\n",
      "['<s> Can: 0.025343189017951427',\n",
      " 'Can I: 0.011829319814110688',\n",
      " 'I do: 0.025565388397246803',\n",
      " 'do my: 0.001893939393939394',\n",
      " 'my DMV: 0.001953125',\n",
      " 'DMV transactions: 0.0016233766233766235',\n",
      " 'transactions online: 0.0010615711252653928',\n",
      " 'online ?: 0.003824904377390565',\n",
      " '? </s>: 0.15963060686015831']\n",
      "\n",
      "Logarithms of bigram conditional probabilities:\n",
      "['<s> Can: -3.6752452628381325',\n",
      " 'Can I: -4.4371740993387805',\n",
      " 'I do: -3.666515858027078',\n",
      " 'do my: -6.269096283706261',\n",
      " 'my DMV: -6.238324625039508',\n",
      " 'DMV transactions: -6.423246963533519',\n",
      " 'transactions online: -6.848005274576363',\n",
      " 'online ?: -5.566221811391143',\n",
      " '? </s>: -1.8348928400456306']\n"
     ]
    }
   ],
   "source": [
    "import numpy.typing as npt\n",
    "import typing\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# Define a function to print the bigram statistics of a tokenized utterance.\n",
    "def print_bigram_statistics(\n",
    "    bigram_matrix: npt.NDArray, tokenized: typing.List[str]\n",
    "):\n",
    "    bigram_statistics = []\n",
    "    for token_current, token_next in zip(\n",
    "        tokenized,\n",
    "        tokenized[1:],\n",
    "    ):\n",
    "        # Find the indices of the tokens in the vocabulary.\n",
    "        token_current_index = find_token_index(token_current, vocab)\n",
    "        token_next_index = find_token_index(token_next, vocab)\n",
    "\n",
    "        # If both tokens are in the vocabulary, find the value of the statistic\n",
    "        # for the bigram and add it to the list to print.\n",
    "        if token_current_index != -1 and token_next_index != -1:\n",
    "            bigram_statistic = bigram_matrix[\n",
    "                token_current_index, token_next_index\n",
    "            ]\n",
    "            bigram_statistics.append(\n",
    "                f\"{token_current} {token_next}: {bigram_statistic}\"\n",
    "            )\n",
    "\n",
    "    pprint(bigram_statistics)\n",
    "\n",
    "\n",
    "# Compute the bigram conditional probabilities.\n",
    "bigram_conditional_probabilities = np.divide(bigram_counts, first_token_counts)\n",
    "\n",
    "print(\"Our results:\")\n",
    "print(\"\\nBigram conditional probabilities:\")\n",
    "print_bigram_statistics(\n",
    "    bigram_conditional_probabilities, doc_tokenized_example_1\n",
    ")\n",
    "\n",
    "# Compute the logarithms of the bigram conditional probabilities.\n",
    "bigram_conditional_log_probabilities = np.log(bigram_conditional_probabilities)\n",
    "\n",
    "print(\"\\nLogarithms of bigram conditional probabilities:\")\n",
    "print_bigram_statistics(\n",
    "    bigram_conditional_log_probabilities, doc_tokenized_example_1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK's result:\n",
      "\n",
      "Bigram probabilities:\n",
      "['<s> Can: 0.015820698747528016',\n",
      " 'Can I: 0.014775725593667546',\n",
      " 'I do: 0.02195945945945946',\n",
      " 'do my: 0.0019656019656019656',\n",
      " 'my DMV: 0.001893043066729768',\n",
      " 'DMV transactions: 0.0014641288433382138',\n",
      " 'transactions online: 0.001081665765278529',\n",
      " 'online ?: 0.004774535809018567',\n",
      " '? </s>: 0.205607476635514']\n",
      "\n",
      "Bigram log probabilities:\n",
      "['<s> Can: -5.982042869525877',\n",
      " 'Can I: -6.0806272110008495',\n",
      " 'I do: -5.5090136474878575',\n",
      " 'do my: -8.990813079153611',\n",
      " 'my DMV: -9.04507705193494',\n",
      " 'DMV transactions: -9.415741768290092',\n",
      " 'transactions online: -9.852529509404196',\n",
      " 'online ?: -7.710423806713715',\n",
      " '? </s>: -2.2820353677638496']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams\n",
    "from pprint import pprint\n",
    "\n",
    "# Compare the result to NLTK's.\n",
    "# TODO: Why are results different?\n",
    "print(\"NLTK's result:\")\n",
    "print(\"\\nBigram probabilities:\")\n",
    "pprint(\n",
    "    [\n",
    "        f\"{token_current} {token_next}: {laplace.score(token_next, [token_current])}\"\n",
    "        for token_current, token_next in list(bigrams(doc_tokenized_example_1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nBigram log probabilities:\")\n",
    "pprint(\n",
    "    [\n",
    "        f\"{token_current} {token_next}: {laplace.logscore(token_next, [token_current])}\"\n",
    "        for token_current, token_next in list(bigrams(doc_tokenized_example_1))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.4:** Write a function that uses log_cond_probs to compute the probability of a given tokenised sentence, such as the example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a tokenized utterance.\n",
    "doc_tokenized_example_2 = [\n",
    "    \"<s>\",\n",
    "    \"If\",\n",
    "    \"you\",\n",
    "    \"give\",\n",
    "    \"me\",\n",
    "    \"the\",\n",
    "    \"help\",\n",
    "    \",\",\n",
    "    \"what\",\n",
    "    \"is\",\n",
    "    \"the\",\n",
    "    \"payment\",\n",
    "    \"system\",\n",
    "    \"?\",\n",
    "    \"<e>\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3643417950084365e-35\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def find_log_probability(\n",
    "    tokenized: typing.List[str],\n",
    "    vocab: typing.List[str],\n",
    "    bigram_matrix: npt.ArrayLike,\n",
    "):\n",
    "    log_probability = 0.0\n",
    "    for token_current, token_next in zip(\n",
    "        tokenized,\n",
    "        tokenized[1:],\n",
    "    ):\n",
    "        # Find the indices of the tokens in the vocabulary.\n",
    "        token_current_index = find_token_index(token_current, vocab)\n",
    "        token_next_index = find_token_index(token_next, vocab)\n",
    "\n",
    "        # If both tokens are in the vocabulary, add to the log probability.\n",
    "        if token_current_index != -1 and token_next_index != -1:\n",
    "            log_probability += bigram_matrix[\n",
    "                token_current_index, token_next_index\n",
    "            ]\n",
    "\n",
    "    return log_probability\n",
    "\n",
    "\n",
    "pprint(\n",
    "    np.exp(\n",
    "        find_log_probability(\n",
    "            doc_tokenized_example_2, vocab, bigram_conditional_log_probabilities\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2.5:** Compute the perplexity over the whole test set. You will need to make sure your code can handle unknown words -- make sure it does not end up misusing the index of -1 returned by get_index_for_word() for unknown words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'Can', 'I', 'do', 'my', 'DMV', 'transactions', 'online', '?', '</s>']]\n",
      "Our result:\n",
      "401.5827718678161\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def find_perplexity(\n",
    "    tokenized: typing.List[str],\n",
    "    vocab: typing.List[str],\n",
    "    bigram_matrix: npt.ArrayLike,\n",
    "):\n",
    "    \"\"\"\n",
    "    The perplexity is the Nth root of the product of the inverse probabilities\n",
    "    of the bigrams, where N is the number of bigrams. Because of the properties\n",
    "    of logarithms, this is equivalent to (pseudocode):\n",
    "    ```\n",
    "        exp(1 - sum(log(probability(bigram)))/N)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    return math.exp(\n",
    "        1\n",
    "        - find_log_probability(tokenized, vocab, bigram_matrix)\n",
    "        / (len(tokenized) - 1)\n",
    "    )\n",
    "\n",
    "\n",
    "# Flatten the test set into a single \"document\".\n",
    "data_test = pad(\n",
    "    [[token.text for token in nlp(\"Can I do my DMV transactions online?\")]]\n",
    ")\n",
    "print(data_test)\n",
    "\n",
    "print(\"Our result:\")\n",
    "pprint(\n",
    "    find_perplexity(data_test[0], vocab, bigram_conditional_log_probabilities)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK's result:\n",
      "909.1327830154821\n"
     ]
    }
   ],
   "source": [
    "print(\"NLTK's result:\")\n",
    "pprint(laplace.perplexity(data_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENSION 1:** Use the language model to generate new sentences by sampling.\n",
    "You can follow the example below to sample using scipy's multinomial class. Replace the distribution with the conditional distribution we computed earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multinomial\n",
    "\n",
    "example_vocab = np.array([\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "distribution = [0.3, 0.2, 0.1, 0.4]\n",
    "sample = multinomial.rvs(1, distribution)\n",
    "sample_bool = sample.astype(bool)  # convert the sample from integer to boolean\n",
    "generated_word = example_vocab[sample_bool][\n",
    "    0\n",
    "]  # use the one-hot boolean vector to look up the word\n",
    "\n",
    "print(generated_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "current_tok = \"<s>\"\n",
    "tokens = [current_tok]\n",
    "\n",
    "while current_tok != \"<e>\" and len(tokens) < 1000:\n",
    "    ### WRITE YOUR CODE HERE\n",
    "\n",
    "    ###\n",
    "    tokens.append(current_tok)\n",
    "\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MORE EXTENSIONS:\n",
    "\n",
    "- Add some smoothing to the counts and see how it affects the results.\n",
    "- Use trigrams instead of bigrams. Does it improve perplexity?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dialogue_and_narrative",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
